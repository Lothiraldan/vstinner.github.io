<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Haypo blog 2</title><link href="https://haypo.github.io/" rel="alternate"></link><link href="https://haypo.github.io/feeds/all.atom.xml" rel="self"></link><id>https://haypo.github.io/</id><updated>2016-11-19T00:30:00+01:00</updated><entry><title>Analysis of a Python performance issue</title><link href="https://haypo.github.io/analysis-python-performance-issue.html" rel="alternate"></link><published>2016-11-19T00:30:00+01:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-11-19:analysis-python-performance-issue.html</id><summary type="html">&lt;p&gt;I am working on the CPython benchmark suite (&lt;a class="reference external" href="https://github.com/python/performance"&gt;performance&lt;/a&gt;) and I run the benchmark suite to
upload results to &lt;a class="reference external" href="http://speed.python.org/"&gt;speed.python.org&lt;/a&gt;. While
analying results, I noticed a temporary peak on the &lt;tt class="docutils literal"&gt;call_method&lt;/tt&gt;
benchmark at October 19th:&lt;/p&gt;
&lt;img alt="call_method microbenchmark" src="https://haypo.github.io/images/call_method.png" /&gt;
&lt;p&gt;The graphic shows the performance of the &lt;tt class="docutils literal"&gt;call_method&lt;/tt&gt; microbenchmark between
Feb 29, 2016 and November 17, 2016 on the &lt;tt class="docutils literal"&gt;default&lt;/tt&gt; branch of CPython. The average
is around 17.2 ms, whereas the peak is at 29.0 ms: &lt;strong&gt;68% slower&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;The server has two &amp;quot;Intel(R) Xeon(R) CPU X5680  &amp;#64; 3.33GHz&amp;quot; CPUs, total: 24
logical cores (12 physical cores with HyperThreading). This CPU was launched in
2010 and based on the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Gulftown"&gt;Westmere-EP microarchitecture&lt;/a&gt;. Westmere-EP is based on Westmere,
which is the 32 nm shrink of the Nehalem microarchitecture.&lt;/p&gt;
&lt;div class="section" id="reproduce-results"&gt;
&lt;h2&gt;Reproduce results&lt;/h2&gt;
&lt;p&gt;Before going too far, the first step is to validate that results are
reproductible: reboot the computer, recompile Python, run again the benchmark.&lt;/p&gt;
&lt;p&gt;Instead of running the full benchmark suite, install Python, ..., we will run
directly the benchmark manually using the Python freshly built in its source
code directory.&lt;/p&gt;
&lt;p&gt;Interesting dots on the graphic (can be seen at speed.python.org, not on the
screenshot):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;678fe178da0d, Oct 09, 17.0 ms: &amp;quot;Fast&amp;quot;&lt;/li&gt;
&lt;li&gt;1ce50f7027c1, Oct 19, 28.9 ms: &amp;quot;Slow&amp;quot;&lt;/li&gt;
&lt;li&gt;36af3566b67a, Nov 3, 16.9 ms: Fast again&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I use the following directories:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;~/perf: GitHub haypo/perf project&lt;/li&gt;
&lt;li&gt;~/performance: GitHub python/performance project&lt;/li&gt;
&lt;li&gt;~/cpython: Mercurial CPython repository&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tune the system for benchmarks:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo python3 -m perf system tune
&lt;/pre&gt;
&lt;p&gt;Note: all &lt;tt class="docutils literal"&gt;system&lt;/tt&gt; commands in this article are optional. They help to reduce
the operating system jitter (make benchmarks more reliablee).&lt;/p&gt;
&lt;p&gt;Fast:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 678fe178da0d
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ mv python python-fast
$ PYTHONPATH=~/perf ./python-fast ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 17.0 ms +- 0.1 ms
&lt;/pre&gt;
&lt;p&gt;Slow:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 1ce50f7027c1
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ mv python python-slow
$ PYTHONPATH=~/perf ./python-slow ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 29.3 ms +- 0.9 ms
&lt;/pre&gt;
&lt;p&gt;We reproduced the significant benchmark result: 17 ms =&amp;gt; 29 ms.&lt;/p&gt;
&lt;p&gt;I use &lt;tt class="docutils literal"&gt;./configure&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;make clean&lt;/tt&gt; instead of incremental compilation,
&lt;tt class="docutils literal"&gt;make&lt;/tt&gt; command, to avoid compilation errors, and to avoid potential side
effects only caused by the incremental compilation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="analysis-with-the-linux-perf-tool"&gt;
&lt;h2&gt;Analysis with the Linux perf tool&lt;/h2&gt;
&lt;p&gt;To collect perf events, we will run the benchmark with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--worker&lt;/span&gt;&lt;/tt&gt; to run a
single process and with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-w0&lt;/span&gt; &lt;span class="pre"&gt;-n100&lt;/span&gt;&lt;/tt&gt; to run the benchmark long enough: 100
samples means at least 10 seconds (a single sample takes at least 100 ms).&lt;/p&gt;
&lt;p&gt;First, reset the system configuration to reset the Linux perf configuration:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo python3 -m perf system reset
&lt;/pre&gt;
&lt;p&gt;Note: &lt;tt class="docutils literal"&gt;python3 &lt;span class="pre"&gt;-m&lt;/span&gt; perf system tune&lt;/tt&gt; reduces the sampling rate of Linux perf
to reduce operating system jitter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="perf-stat"&gt;
&lt;h2&gt;perf stat&lt;/h2&gt;
&lt;p&gt;Command to get general statistics on the benchmark:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ perf stat ./python-slow ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --worker -v -w0 -n100
&lt;/pre&gt;
&lt;p&gt;&amp;quot;Fast&amp;quot; results:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Performance counter stats for ./python-fast:

      3773.585194 task-clock (msec)         #    0.998 CPUs utilized
              369 context-switches          #    0.098 K/sec
                0 cpu-migrations            #    0.000 K/sec
            8,300 page-faults               #    0.002 M/sec
   12,981,234,867 cycles                    #    3.440 GHz                     [83.27%]
    1,460,980,720 stalled-cycles-frontend   #   11.25% frontend cycles idle    [83.36%]
      435,806,788 stalled-cycles-backend    #    3.36% backend  cycles idle    [66.72%]
   29,982,530,201 instructions              #    2.31  insns per cycle
                                            #    0.05  stalled cycles per insn [83.40%]
    5,613,631,616 branches                  # 1487.612 M/sec                   [83.40%]
       16,006,564 branch-misses             #    0.29% of all branches         [83.27%]

      3.780064486 seconds time elapsed
&lt;/pre&gt;
&lt;p&gt;&amp;quot;Slow&amp;quot; results:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Performance counter stats for ./python-slow:

      5906.239860 task-clock (msec)         #    0.998 CPUs utilized
              556 context-switches          #    0.094 K/sec
                0 cpu-migrations            #    0.000 K/sec
            8,393 page-faults               #    0.001 M/sec
   20,651,474,102 cycles                    #    3.497 GHz                     [83.36%]
    8,480,803,345 stalled-cycles-frontend   #   41.07% frontend cycles idle    [83.37%]
    4,247,826,420 stalled-cycles-backend    #   20.57% backend  cycles idle    [66.64%]
   30,011,465,614 instructions              #    1.45  insns per cycle
                                            #    0.28  stalled cycles per insn [83.32%]
    5,612,485,730 branches                  #  950.264 M/sec                   [83.36%]
       13,584,136 branch-misses             #    0.24% of all branches         [83.29%]

      5.915402403 seconds time elapsed
&lt;/pre&gt;
&lt;p&gt;Significant differences, Fast =&amp;gt; Slow:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Instruction per cycle: 2.31 =&amp;gt; 1.45&lt;/li&gt;
&lt;li&gt;stalled-cycles-frontend: &lt;strong&gt;11.25% =&amp;gt; 41.07%&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;stalled-cycles-backend: &lt;strong&gt;3.36% =&amp;gt; 20.57%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The increase of stalled cycles is interesting. Since the code is supposed to be
identical, it probably means that fetching instructions is slower. It sounds
like an issue with CPU caches.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="statistics-on-the-cpu-l1-instruction-cache"&gt;
&lt;h2&gt;Statistics on the CPU L1 instruction cache&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf list&lt;/tt&gt; command can be used to get the name of events collecting
statistics on the CPU L1 instruction cache:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ perf list | grep L1
  L1-icache-loads                                    [Hardware cache event]
  L1-icache-load-misses                              [Hardware cache event]
  (...)
&lt;/pre&gt;
&lt;p&gt;Collect statistics on the CPU L1 instruction cache:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PYTHONPATH=~/perf perf stat -e L1-icache-loads,L1-icache-load-misses ./python-slow ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --worker -w0 -n10
&lt;/pre&gt;
&lt;p&gt;&amp;quot;Fast&amp;quot; statistics:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Performance counter stats for './python-fast (...)':

   10,134,106,571 L1-icache-loads
       10,917,606 L1-icache-load-misses     #    0.11% of all L1-icache hits

      3.775067668 seconds time elapsed
&lt;/pre&gt;
&lt;p&gt;&amp;quot;Slow&amp;quot; statistics:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Performance counter stats for './python-slow (...)':

   10,753,371,258 L1-icache-loads
      848,511,308 L1-icache-load-misses     #    7.89% of all L1-icache hits

      6.020490449 seconds time elapsed
&lt;/pre&gt;
&lt;p&gt;Cache misses on the L1 cache: &lt;strong&gt;0.1%&lt;/strong&gt; (Fast) =&amp;gt; &lt;strong&gt;8.0%&lt;/strong&gt; (Slow).&lt;/p&gt;
&lt;p&gt;The slow Python has &lt;strong&gt;71.7x more L1 cache misses&lt;/strong&gt; than the fast Python! It can
explain the significant performance drop.&lt;/p&gt;
&lt;div class="section" id="perf-report"&gt;
&lt;h3&gt;perf report&lt;/h3&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf record&lt;/tt&gt; command can be used to collect statistics on the functions
where the benchmark spends most of its time. Commands:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PYTHONPATH=~/perf perf record ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --worker -v -w0 -n100
perf report
&lt;/pre&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
40.27%  python  python              [.] _PyEval_EvalFrameDefault
10.30%  python  python              [.] call_function
10.21%  python  python              [.] PyFrame_New
 8.56%  python  python              [.] frame_dealloc
 5.51%  python  python              [.] PyObject_GenericGetAttr
 (...)
&lt;/pre&gt;
&lt;p&gt;More than 64% of the time is spent in these 5 functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="system-tune"&gt;
&lt;h3&gt;system tune&lt;/h3&gt;
&lt;p&gt;To run benchmark, tune again the system for benchmarks:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo python3 -m perf system tune
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="hg-bisect"&gt;
&lt;h2&gt;hg bisect&lt;/h2&gt;
&lt;p&gt;To find the revision which introduces the performance slowdown, we use a
shell script to automate the bisection of the Mercurial history.&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;cmd.sh&lt;/tt&gt; script checking if a revision is fast or slow:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
set -e -x
./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
rm -f json
PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --worker -o json -v
PYTHONPATH=~/perf python3 cmd.py json
&lt;/pre&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;cmd.sh&lt;/tt&gt; uses the following &lt;tt class="docutils literal"&gt;cmd.py&lt;/tt&gt; script which checks if the benchmark
is slow: if it takes longer than 23 ms (average between 17 ans 29 ms):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
import perf, sys
bench = perf.Benchmark.load('json')
bad = (29 + 17) / 2.0
ms = bench.median() * 1e3
if ms &amp;gt;= bad:
    print(&amp;quot;BAD! %.1f ms &amp;gt;= %.1f ms&amp;quot; % (ms, bad))
    sys.exit(1)
else:
    print(&amp;quot;good: %.1f ms &amp;lt; %.1f ms&amp;quot; % (ms, bad))
&lt;/pre&gt;
&lt;p&gt;In the bisection, &amp;quot;good&amp;quot; means &amp;quot;fast&amp;quot; (17 ms), whereas &amp;quot;bad&amp;quot; means &amp;quot;slow&amp;quot; (29
ms).  The peak, revision 1ce50f7027c1, is used as the first &amp;quot;bad&amp;quot; revision. The
previous fast revision before the peak is 678fe178da0d, our first &amp;quot;good&amp;quot;
revision.&lt;/p&gt;
&lt;p&gt;Commands to identify the first revision which introduced the slowdown:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
hg bisect --reset
hg bisect -b 1ce50f7027c1
hg bisect -g 678fe178da0d
time hg bisect -c ./cmd.sh
&lt;/pre&gt;
&lt;p&gt;3 min 52 sec later:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
The first bad revision is:
changeset:   104531:83877018ef97
parent:      104528:ce85a1f129e3
parent:      104530:2d352bf2b228
user:        Serhiy Storchaka &amp;lt;storchaka&amp;#64;gmail.com&amp;gt;
date:        Tue Oct 18 13:27:54 2016 +0300
files:       Misc/NEWS
description:
Issue #23782: Fixed possible memory leak in _PyTraceback_Add() and exception
loss in PyTraceBack_Here().
&lt;/pre&gt;
&lt;p&gt;Thank you &lt;tt class="docutils literal"&gt;hg bisect&lt;/tt&gt;! I love this tool.&lt;/p&gt;
&lt;p&gt;Even if I trust &lt;tt class="docutils literal"&gt;hg bisect&lt;/tt&gt;, I don't trust benchmarks, so I recheck manually:&lt;/p&gt;
&lt;p&gt;Slow:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 83877018ef97
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 29.4 ms +- 1.8 ms
&lt;/pre&gt;
&lt;p&gt;Use &lt;tt class="docutils literal"&gt;hg parents&lt;/tt&gt; to get the latest fast revision:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg parents -r 83877018ef97
changeset:   104528:ce85a1f129e3
(...)

changeset:   104530:2d352bf2b228
branch:      3.6
(...)
&lt;/pre&gt;
&lt;p&gt;Check the parent:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r ce85a1f129e3
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 17.1 ms +- 0.1 ms
&lt;/pre&gt;
&lt;p&gt;The revision ce85a1f129e3 is fast and the following revision 83877018ef97 is
slow. &lt;strong&gt;The revision 83877018ef97 introduced the slowdown&lt;/strong&gt;.  We found it!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="analysis-of-the-revision-introducing-the-slowdown"&gt;
&lt;h2&gt;Analysis of the revision introducing the slowdown&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://hg.python.org/cpython/rev/83877018ef97/"&gt;revision 83877018ef97&lt;/a&gt;
changes two files: Misc/NEWS and Python/traceback.c. The NEWS file is only
documentation and so must not impact performances.  Python/traceback.c is part
of the C code and so is more interesting.&lt;/p&gt;
&lt;p&gt;The commit only changes two C functions: &lt;tt class="docutils literal"&gt;PyTraceBack_Here()&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;_PyTraceback_Add()&lt;/tt&gt;, but &lt;tt class="docutils literal"&gt;perf report&lt;/tt&gt; didn't show these functions as &amp;quot;hot&amp;quot;.
In fact, these functions are never called by the benchmark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The commit doesn't touch the C code used in the benchmark.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Unrelated C change impacting performances reminds me my previous &lt;a class="reference external" href="https://haypo.github.io/journey-to-stable-benchmark-deadcode.html"&gt;deadcode
horror story&lt;/a&gt;. The performance
difference is probably caused by &lt;strong&gt;&amp;quot;code placement&amp;quot;&lt;/strong&gt;: &lt;tt class="docutils literal"&gt;perf stat&lt;/tt&gt; showed a
significant increase of the cache miss rate on the L1 instruction cache.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="use-gcc-attribute-hot"&gt;
&lt;h2&gt;Use GCC __attribute__((hot))&lt;/h2&gt;
&lt;p&gt;Using PGO compilation was the solution for deadcode, but PGO doesn't work on
Ubuntu 14.04 (the OS used by the benchmark server, speed-python) and PGO seems
to make benchmarks less reliable.&lt;/p&gt;
&lt;p&gt;I wanted to try something else: mark hot functions using the GCC
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;__attribute__((hot))&lt;/span&gt;&lt;/tt&gt; attribute. PGO compilation does this automatically.&lt;/p&gt;
&lt;p&gt;This attribute only has an impact on the code placement: where functions are
loaded in memory. The flag declares functions in the &lt;tt class="docutils literal"&gt;.text.hot&lt;/tt&gt; ELF section
rather than the &lt;tt class="docutils literal"&gt;.text&lt;/tt&gt; ELF section. Grouping hot functions in the same
functions helps to reduce the distance between functions and so enhance the
usage of CPU caches.&lt;/p&gt;
&lt;p&gt;I wrote and then pushed a patch in the &lt;a class="reference external" href="http://bugs.python.org/issue28618"&gt;issue #28618&lt;/a&gt;: &amp;quot;Decorate hot functions using
__attribute__((hot)) to optimize Python&amp;quot;.&lt;/p&gt;
&lt;p&gt;The patch marks 6 functions as hot:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;_PyEval_EvalFrameDefault()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;call_function()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;_PyFunction_FastCall()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;PyFrame_New()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;frame_dealloc()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;PyErr_Occurred()&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's try the patch:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 83877018ef97
$ wget https://hg.python.org/cpython/raw-rev/59b91b4e9506 -O patch
$ patch -p1 &amp;lt; patch
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 16.7 ms +- 0.3 ms
&lt;/pre&gt;
&lt;p&gt;It's easy to make mistakes and benchmarks are always suprising, so let's retry
without the patch:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 83877018ef97
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 29.3 ms +- 0.6 ms
&lt;/pre&gt;
&lt;p&gt;The check confirms that the GCC attribute fixed the issue!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;On modern Intel CPUs, the code placement can have a major impact on the
performance of microbenchmarks.&lt;/p&gt;
&lt;p&gt;The GCC &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;__attribute__((hot))&lt;/span&gt;&lt;/tt&gt; attribute can be used manually to make &amp;quot;hot
functions&amp;quot; close in memory to enhance the usage of CPU caches.&lt;/p&gt;
&lt;p&gt;To know more about the impact of code placement, see the very good talk of Zia
Ansari (Intel) at the LLVM Developers' Meeting 2016: &lt;a class="reference external" href="https://llvmdevelopersmeetingbay2016.sched.org/event/8YzY/causes-of-performance-instability-due-to-code-placement-in-x86"&gt;Causes of Performance
Swings Due to Code Placement in IA&lt;/a&gt;.
He describes well &amp;quot;performance swings&amp;quot; like the one described in this article
and explains how CPUs work internally and how code placement impacts CPU
performances.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="optimization"></category><category term="benchmark"></category></entry><entry><title>Intel CPUs (part 2): Turbo Boost, temperature, frequency and Pstate C0 bug</title><link href="https://haypo.github.io/intel-cpus-part2.html" rel="alternate"></link><published>2016-09-23T23:00:00+02:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-09-23:intel-cpus-part2.html</id><summary type="html">&lt;p&gt;My first article &lt;a class="reference external" href="https://haypo.github.io/intel-cpus.html"&gt;Intel CPUs&lt;/a&gt; is a general
introduction on modern CPU technologies having an impact on benchmarks.&lt;/p&gt;
&lt;p&gt;This second article is much more concrete with numbers and a concrete bug
having a major impact on benchmarks: a benchmark suddenly becomes 2x faster!&lt;/p&gt;
&lt;p&gt;I will tell you how I first noticed the bug, which tests I ran to analyze the
issue, how I found commands to reproduce the bug, and finally how I identified
the bug.&lt;/p&gt;
&lt;div class="section" id="glitch-in-benchmarks"&gt;
&lt;h2&gt;&amp;quot;Glitch&amp;quot; in benchmarks&lt;/h2&gt;
&lt;p&gt;Last week I ran a benchmark to check if enabling Profile Guided Optimization
(PGO) when compiling Python makes benchmark results less stable. I recompiled
Python 5 times, and after each compilation I ran a benchmark. I tested
different commands and options to compile Python. Everything was fine until
the last benchmark of the last compilation. &lt;strong&gt;The benchmark suddenly became 2
times faster.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hopefully, my perf module collects a lot of metadata. I was able to analyze
in depth what happened.&lt;/p&gt;
&lt;p&gt;The &amp;quot;glitch&amp;quot; occurred in a benchmark having 400 runs (benchmark run in 400
different processes), between the run 105 (20.3 ms) and the run 106
(11.0 ms).&lt;/p&gt;
&lt;p&gt;I noticed that the CPU temperature was between 69°C and 72°C until the run 105,
and then decreased to from 69°C to 58°C.&lt;/p&gt;
&lt;p&gt;The system load slowly increased from 1.25 up to 1.62 around the run 108 and
then slowly decreased to 1.00.&lt;/p&gt;
&lt;p&gt;The system was not idle while the benchmark was running. I was working on the
PC too! But according to timestamps, it seems like the glitch was close to when
I stopped working. When I stopped working, I closed all applications (except of
the benchmark running in background) and turned of my two monitors.&lt;/p&gt;
&lt;p&gt;Well, at this point, it's hard to correlate for sure an event with the major
performance change.&lt;/p&gt;
&lt;p&gt;So I started to analyze different factors affecting CPUs and benchmarks: Turbo
Boost, CPU temperature and CPU frequency.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="impact-of-turbo-boost-on-benchmarks"&gt;
&lt;h2&gt;Impact of Turbo Boost on benchmarks&lt;/h2&gt;
&lt;p&gt;Without Turbo Boost, the maximum frequency of the &amp;quot;Intel(R) Core(TM) i7-3520M
CPU &amp;#64; 2.90GHz&amp;quot; of my laptop is 2.9 GHz. With Turbo Boost, the maximum
frequency is 3.6 GHz if only one core is active, or 3.4 GHz otherwise:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ sudo cpupower frequency-info
  ...
  boost state support:
    Supported: yes
    Active: yes
    3400 MHz max turbo 4 active cores
    3400 MHz max turbo 3 active cores
    3400 MHz max turbo 2 active cores
    3600 MHz max turbo 1 active cores
&lt;/pre&gt;
&lt;p&gt;I ran the bm_call_simple.py microbenchmark (CPU-bound) of performance 0.2.2.&lt;/p&gt;
&lt;p&gt;Turbo Boost disabled:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;1 physical CPU active: 2.9 GHz, Median +- std dev: 14.6 ms +- 0.3 ms&lt;/li&gt;
&lt;li&gt;2 physical CPU active: 2.9 GHz, Median +- std dev: 14.7 ms +- 0.5 ms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Turbo Boost enabled:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;1 physical CPU active: 3.6 GHz, Median +- std dev: 11.8 ms +- 0.3 ms&lt;/li&gt;
&lt;li&gt;2 physical CPU active: 3.4 GHz, Median +- std dev: 12.4 ms +- 0.1 ms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The maximum performance boost is 19% faster&lt;/strong&gt; (14.6 ms =&amp;gt; 11.8 ms), the
minimum boost if 15% faster (14.6 ms =&amp;gt; 12.4 ms).&lt;/p&gt;
&lt;p&gt;Hum, I don't think that Turbo Boost can explain the bug.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="impact-of-the-cpu-temperature-on-benchmarks"&gt;
&lt;h2&gt;Impact of the CPU temperature on benchmarks&lt;/h2&gt;
&lt;p&gt;The CPU temperature is mentionned in Intel Turbo Boost documentation as a
factor used to decide which P-state will be used. I always wanted to check how
the CPU temperature impacts its performance.&lt;/p&gt;
&lt;div class="section" id="burn-the-cpu-of-my-desktop-pc"&gt;
&lt;h3&gt;Burn the CPU of my desktop PC&lt;/h3&gt;
&lt;p&gt;CPU of my desktop PC: &amp;quot;Intel(R) Core(TM) i7-2600 CPU &amp;#64; 3.40GHz&amp;quot;.&lt;/p&gt;
&lt;p&gt;I used my &lt;a class="reference external" href="https://bitbucket.org/haypo/misc/src/tip/bin/system_load.py"&gt;system_load.py script&lt;/a&gt; to generate a
system load higher than 10.&lt;/p&gt;
&lt;p&gt;When the fan is cooling correctly the CPU, all CPU run at 3.4 GHz (Turbo Boost
was disabled) and the CPU temperature is 66°C.&lt;/p&gt;
&lt;p&gt;I used a simple sheet of paper to block the fan of my CPU. Yeah, I really
wanted to &lt;a class="reference external" href="https://www.youtube.com/watch?v=Xf0VuRG7MN4"&gt;burn my CPU&lt;/a&gt;! More
seriously, I checked the CPU temperature every second using the &lt;tt class="docutils literal"&gt;sensors&lt;/tt&gt;
command and was prepared to unblock the fan if sometimes gone wrong.&lt;/p&gt;
&lt;img alt="Sheet of paper blocking the CPU fan" src="https://haypo.github.io/images/paper_blocks_cpu_fan.jpg" /&gt;
&lt;p&gt;After one minute, the CPU reached 97°C. I expected a system crash, smoke or
something worse, but I was disappointed. &lt;strong&gt;At 97°C, I was still able to use my
computer as everything was fine. The CPU was slowly down automatically to the
minimum CPU frequency: 1533 MHz&lt;/strong&gt; according to turbostat (the minimum frequency
of this CPU is 1.6 GHz).&lt;/p&gt;
&lt;p&gt;When I unblocked the fan, the temperature decreased quickly to go back to its
previous state (62°C) and the CPU frequency quickly increased to 3.4 GHz as
well.&lt;/p&gt;
&lt;p&gt;My Intel CPU is really impressive! I didn't expect such very efficient
protection against overheating!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="burn-my-laptop-cpu"&gt;
&lt;h3&gt;Burn my laptop CPU&lt;/h3&gt;
&lt;p&gt;I used my system_load.py script to get a system load over 200. I also opened 4
tabs in Firefox playing Youtube videos to stress also the GPU which is
integrated into the CPU (IGP) on such laptop.&lt;/p&gt;
&lt;img alt="Stress test playing Youtube videos in Firefox, CPU at 102°" src="https://haypo.github.io/images/burn_cpu_firefox.jpg" /&gt;
&lt;p&gt;With such crazy stress test, the CPU temperature was &amp;quot;only&amp;quot; 83°C.&lt;/p&gt;
&lt;p&gt;Using a simple tissue, I closed the air hole used by the CPU fan. &lt;strong&gt;When the
CPU temperature increased from 100°C to 101°C, the CPU frequency started slowly
to decrease from 3391 MHz to 3077 MHz&lt;/strong&gt; (with steps between 10 MHz and 50 MHz
every second, or something like that).&lt;/p&gt;
&lt;p&gt;When pushing hard the tissue and waiting longer than 5 minutes, the CPU
temperature increased up to 102°C, but the CPU frequency was only decreased
from 3.4 GHz (Turbo Mode with 4 active logical CPUs) to 3.1 GHz.&lt;/p&gt;
&lt;p&gt;The maximum frequency is 2.9 GHz. Frequencies higher than 2.9 GHz means that
the Turbo Mode was enabled! It means that &lt;strong&gt;even with overheating, the CPU is
still fine and able to &amp;quot;overclock&amp;quot; itself!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Again, I was disapointed. With a CPU at 102°C, my laptop was still super fast
and reactive.  It seems like mobile CPUs handle even better overheating than
desktop CPUs (which is not something suprising at all).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="impact-of-the-cpu-frequency-on-benchmarks"&gt;
&lt;h2&gt;Impact of the CPU frequency on benchmarks&lt;/h2&gt;
&lt;p&gt;I ran the bm_call_simple.py microbenchmark (CPU-bound) of performance 0.2.2
on my desktop PC.&lt;/p&gt;
&lt;p&gt;Command to set the frequency of CPU 0 to the minimum frequency (1.6 GHz):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_min_freq|sudo tee  /sys/devices/system/cpu/cpu0/cpufreq/scaling_max_freq
1600000
&lt;/pre&gt;
&lt;p&gt;Command to set the frequency of CPU 0 to the maximum frequency (3.4 GHz):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq|sudo tee  /sys/devices/system/cpu/cpu0/cpufreq/scaling_max_freq
3400000
&lt;/pre&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;CPU running at 1.6 GHz (min freq): Median +- std dev: 27.7 ms +- 0.7 ms&lt;/li&gt;
&lt;li&gt;CPU running at 3.4 GHz (min freq): Median +- std dev: 12.9 ms +- 0.2 ms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The impact of the CPU frequency is quite obvious: &lt;strong&gt;when the CPU frequency is
doubled, the performance is also doubled&lt;/strong&gt;. The benchmark is 53% faster (27.7
ms =&amp;gt; 12.9 ms).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="bug-reproduced-and-then-identified-in-the-linux-cpu-driver"&gt;
&lt;h2&gt;Bug reproduced and then identified in the Linux CPU driver&lt;/h2&gt;
&lt;p&gt;Two days ago, I ran a very simple &amp;quot;timeit&amp;quot; microbenchmark to try to bisect a
performance regression in Python 3.6 on &lt;tt class="docutils literal"&gt;functools.partial&lt;/tt&gt;. Again, suddenly,
the microbenchmark became 2x faster!&lt;/p&gt;
&lt;p&gt;But this time, I found something: I noticed that running or stopping &lt;tt class="docutils literal"&gt;cpupower
monitor&lt;/tt&gt; and/or &lt;tt class="docutils literal"&gt;turbostat&lt;/tt&gt; can &amp;quot;enable&amp;quot; or &amp;quot;disable&amp;quot; the bug.&lt;/p&gt;
&lt;p&gt;After a lot of tests, I understood that running the benchmark with turbostat
&amp;quot;disables&amp;quot; the bug, whereas running &amp;quot;cpupower monitor&amp;quot; while running a
benchmark enables the bug.&lt;/p&gt;
&lt;p&gt;I reported the bug in the Fedora bug tracker, on the component kernel:
&lt;a class="reference external" href="https://bugzilla.redhat.com/show_bug.cgi?id=1378529"&gt;intel_pstate C0 bug on isolated CPUs with the performance governor and
NOHZ_FULL&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It seems like the bug is related to CPU isolation and NOHZ_FULL. The NOHZ_FULL
option is able to fully disable the scheduler clock interruption  on isolated
CPUs. I understood the the &lt;tt class="docutils literal"&gt;intel_pstate&lt;/tt&gt; driver uses a callback on the
scheduler to update the Pstate of the CPU. According to an Intel engineer, the
&lt;tt class="docutils literal"&gt;intel_pstate&lt;/tt&gt; driver was never tested with CPU isolation.&lt;/p&gt;
&lt;p&gt;The issue is not fully analyzed yet, but at least I succeeded to write a list
of commands to reproduce it with a success rate of 100% :-) Moreover, the Intel
engineer suggested to add an extra parameter to the Linux kernel command
(&lt;tt class="docutils literal"&gt;rcu_nocbs=3,7&lt;/tt&gt;) line which works around the issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This article describes how I found and then identified a bug in the Linux
driver of my CPU.&lt;/p&gt;
&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The maximum speedup of Turbo Boost is 20%&lt;/li&gt;
&lt;li&gt;Overheating on a dekstop PC can decrease the CPU frequency to its minimum
(half of the maximum in my case) which imply a slowdown of 50%&lt;/li&gt;
&lt;li&gt;A bug in the Linux CPU driver changes suddenly the CPU frequency from its
minimum to maximum (or the opposite) which means a speedup of 50%
(or slowdown of 50%)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;To get stable benchmarks, the safest fix for all these issues is probably to
set the CPU frequency of the CPUs used by benchmarks to the minimum.&lt;/strong&gt;
It seems like nothing can reduce the frequency of a CPU below its minimum.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When running benchmarks, raw timings and CPU performance don't matter. Only
comparisons between benchmark results and stable performances matter.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</summary><category term="optimization"></category><category term="benchmark"></category><category term="cpu"></category></entry><entry><title>Intel CPUs: P-state, C-state, Turbo Boost, CPU frequency, etc.</title><link href="https://haypo.github.io/intel-cpus.html" rel="alternate"></link><published>2016-07-15T12:00:00+02:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-07-15:intel-cpus.html</id><summary type="html">&lt;p&gt;Ten years ago, most computers were desktop computers designed for best
performances and their CPU frequency was fixed. Nowadays, most devices are
embedded and use &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Low-power_electronics"&gt;low power consumption&lt;/a&gt; processors like ARM
CPUs. The power consumption now matters more than performance peaks.&lt;/p&gt;
&lt;p&gt;Intel CPUs evolved from a single core to multiple physical cores in the same
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/CPU_socket"&gt;package&lt;/a&gt; and got new features:
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Hyper-threading"&gt;Hyper-threading&lt;/a&gt; to run two
threads on the same physical core and &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Intel_Turbo_Boost"&gt;Turbo Boost&lt;/a&gt; to maximum performances.
CPU cores can be completely turned off (CPU HALT, frequency of 0) temporarily to
reduce the power consumption, and the frequency of cores changes regulary
depending on many factors like the workload and temperature. The power
consumption is now an important part in the design of modern CPUs.&lt;/p&gt;
&lt;p&gt;Warning! This article is a summary of what I learnt last weeks from random
articles. It may be full of mistakes, don't hesitate to report them, so I can
enhance the article! It's hard to find simple articles explaining performances
of modern Intel CPUs, so I tried to write mine.&lt;/p&gt;
&lt;div class="section" id="tools-used-in-this-article"&gt;
&lt;h2&gt;Tools used in this article&lt;/h2&gt;
&lt;p&gt;This article mentions various tools. Commands to install them on Fedora 24:&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;dnf install &lt;span class="pre"&gt;-y&lt;/span&gt; &lt;span class="pre"&gt;util-linux&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;lscpu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;dnf install &lt;span class="pre"&gt;-y&lt;/span&gt; &lt;span class="pre"&gt;kernel-tools&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://linux.die.net/man/1/cpupower"&gt;cpupower&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;turbostat&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;sudo dnf install &lt;span class="pre"&gt;-y&lt;/span&gt; &lt;span class="pre"&gt;msr-tools&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;rdmsr&lt;/li&gt;
&lt;li&gt;wrmsr&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other interesting tools, not used in this article: i7z (sadly no more
maintained), lshw, dmidecode, sensors.&lt;/p&gt;
&lt;p&gt;The sensors tool is supposed to report the current CPU voltage, but it doesn't
provide this information on my computers. At least, it gives the temperature of
different components, but also the speed of fans.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example-of-intel-cpus"&gt;
&lt;h2&gt;Example of Intel CPUs&lt;/h2&gt;
&lt;div class="section" id="my-laptop-cpu-proc-cpuinfo"&gt;
&lt;h3&gt;My laptop CPU: /proc/cpuinfo&lt;/h3&gt;
&lt;p&gt;On Linux, the most common way to retrieve information on the CPU is to read
&lt;tt class="docutils literal"&gt;/proc/cpuinfo&lt;/tt&gt;. Example on my laptop:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cat /proc/cpuinfo
processor  : 0
vendor_id  : GenuineIntel
model name : Intel(R) Core(TM) i7-3520M CPU &amp;#64; 2.90GHz
cpu MHz    : 1200.214
...

processor  : 1
vendor_id  : GenuineIntel
model name : Intel(R) Core(TM) i7-3520M CPU &amp;#64; 2.90GHz
cpu MHz    : 3299.882
...
&lt;/pre&gt;
&lt;p&gt;&amp;quot;i7-3520M&amp;quot; CPU is a model designed for Mobile Platforms (see the &amp;quot;M&amp;quot; suffix).
It was built in 2012 and is the third generation of the Intel i7
microarchitecture: &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Ivy_Bridge_(microarchitecture)"&gt;Ivy Bridge&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The CPU has two physical cores, I disabled HyperThreading in the BIOS.&lt;/p&gt;
&lt;p&gt;The first strange thing is that the CPU announces &amp;quot;2.90 GHz&amp;quot; but Linux reports
1.2 GHz on the first core, and 3.3 GHz on the second core. 3.3 GHz is greater
than 2.9 GHz!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="my-desktop-cpu-cpu-topology-with-lscpu"&gt;
&lt;h3&gt;My desktop CPU: CPU topology with lscpu&lt;/h3&gt;
&lt;p&gt;cpuinfo:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
smithers$ cat /proc/cpuinfo
processor   : 0
physical id : 0
core id     : 0
...
model name  : Intel(R) Core(TM) i7-2600 CPU &amp;#64; 3.40GHz
cpu cores   : 4
...

processor   : 1
physical id : 0
core id     : 1
...

(...)

processor   : 7
physical id : 0
core id     : 3
...
&lt;/pre&gt;
&lt;p&gt;The CPU i7-2600 is the 2nd generation: &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sandy_Bridge"&gt;Sandy Bridge microarchitecture&lt;/a&gt;. There are 8 logical cores and 4
physical cores (so with Hyper-threading).&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;lscpu&lt;/tt&gt; renders a short table which helps to understand the CPU topology:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
smithers$ lscpu -a -e
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE MAXMHZ    MINMHZ
0   0    0      0    0:0:0:0       yes    3800.0000 1600.0000
1   0    0      1    1:1:1:0       yes    3800.0000 1600.0000
2   0    0      2    2:2:2:0       yes    3800.0000 1600.0000
3   0    0      3    3:3:3:0       yes    3800.0000 1600.0000
4   0    0      0    0:0:0:0       yes    3800.0000 1600.0000
5   0    0      1    1:1:1:0       yes    3800.0000 1600.0000
6   0    0      2    2:2:2:0       yes    3800.0000 1600.0000
7   0    0      3    3:3:3:0       yes    3800.0000 1600.0000
&lt;/pre&gt;
&lt;p&gt;There are 8 logical CPUs (&lt;tt class="docutils literal"&gt;CPU &lt;span class="pre"&gt;0..7&lt;/span&gt;&lt;/tt&gt;), all on the same node (&lt;tt class="docutils literal"&gt;NODE 0&lt;/tt&gt;) and
the same socket (&lt;tt class="docutils literal"&gt;SOCKET 0&lt;/tt&gt;).  There are only 4 physical cores (&lt;tt class="docutils literal"&gt;CORE
&lt;span class="pre"&gt;0..3&lt;/span&gt;&lt;/tt&gt;). For example, the physical core &lt;tt class="docutils literal"&gt;2&lt;/tt&gt; is made of the two logical CPUs:
&lt;tt class="docutils literal"&gt;2&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;6&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;Using the &lt;tt class="docutils literal"&gt;L1d:L1i:L2:L3&lt;/tt&gt; column, we can see that each pair of two logical
cores share the same physical core caches for levels 1 (L1 data, L1
instruction) and 2 (L2).  All physical cores share the same cache level 3 (L3).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="p-states"&gt;
&lt;h2&gt;P-states&lt;/h2&gt;
&lt;p&gt;A new CPU driver &lt;tt class="docutils literal"&gt;intel_pstate&lt;/tt&gt; was added to the Linux kernel 3.9 (April
2009). First, it only supported SandyBridge CPUs (2nd generation), Linux 3.10
extended it to Ivybridge generation CPUs (3rd gen), and so on and so forth.&lt;/p&gt;
&lt;p&gt;This driver supports recent features and thermal control of modern Intel CPUs.
Its name comes from P-states.&lt;/p&gt;
&lt;p&gt;The processor P-state is the capability of running the processor at different
voltage and/or frequency levels. Generally, P0 is the highest state resulting
in maximum performance, while P1, P2, and so on, will save power but at some
penalty to CPU performance.&lt;/p&gt;
&lt;p&gt;It is possible to force the legacy CPU driver (&lt;tt class="docutils literal"&gt;acpi_cpufreq&lt;/tt&gt;) using
&lt;tt class="docutils literal"&gt;intel_pstate=disable&lt;/tt&gt; option in the kernel command line.&lt;/p&gt;
&lt;p&gt;See also:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.kernel.org/doc/Documentation/cpu-freq/intel-pstate.txt"&gt;Documentation of the intel-pstate driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://plus.google.com/+ArjanvandeVen/posts/dLn9T4ehywL"&gt;Some basics on CPU P states on Intel processors&lt;/a&gt; (2013) by Arjan
van de Ven (Intel)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://events.linuxfoundation.org/sites/events/files/slides/LinuxConEurope_2015.pdf"&gt;Balancing Power and Performance in the Linux Kernel&lt;/a&gt;
talk at LinuxCon Europe 2015 by Kristen Accardi (Intel)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://software.intel.com/en-us/blogs/2008/05/29/what-exactly-is-a-p-state-pt-1"&gt;What exactly is a P-state? (Pt. 1)&lt;/a&gt;
(2008) by Taylor K. (Intel)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="idle-states-c-states"&gt;
&lt;h2&gt;Idle states: C-states&lt;/h2&gt;
&lt;p&gt;C-states are idle power saving states, in contrast to P-states, which are
execution power saving states.&lt;/p&gt;
&lt;p&gt;During a P-state, the processor is still executing instructions, whereas during
a C-state (other than C0), the processor is idle, meaning that nothing is
executing.&lt;/p&gt;
&lt;p&gt;C-states:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;C0 is the operational state, meaning that the CPU is doing useful work&lt;/li&gt;
&lt;li&gt;C1 is the first idle state&lt;/li&gt;
&lt;li&gt;C2 is the second idle state: The external I/O Controller Hub blocks
interrupts to the processor.&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When a logical processor is idle (C-state except of C0), its frequency is
typically 0 (HALT).&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;cpupower &lt;span class="pre"&gt;idle-info&lt;/span&gt;&lt;/tt&gt; command lists supported C-states:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cpupower idle-info
CPUidle driver: intel_idle
CPUidle governor: menu
analyzing CPU 0:

Number of idle states: 6
Available idle states: POLL C1-IVB C1E-IVB C3-IVB C6-IVB C7-IVB
...
&lt;/pre&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;cpupower monitor&lt;/tt&gt; shows statistics on C-states:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
smithers$ sudo cpupower monitor -m Idle_Stats
    |Idle_Stats
CPU | POLL | C1-S | C1E- | C3-S | C6-S
   0|  0,00|  0,19|  0,09|  0,58| 96,23
   4|  0,00|  0,00|  0,00|  0,00| 99,90
   1|  0,00|  2,34|  0,00|  0,00| 97,63
   5|  0,00|  0,00|  0,17|  0,00| 98,02
   2|  0,00|  0,00|  0,00|  0,00|  0,00
   6|  0,00|  0,00|  0,00|  0,00|  0,00
   3|  0,00|  0,00|  0,00|  0,00|  0,00
   7|  0,00|  0,00|  0,00|  0,00| 49,97
&lt;/pre&gt;
&lt;p&gt;See also: &lt;a class="reference external" href="https://software.intel.com/en-us/articles/power-management-states-p-states-c-states-and-package-c-states"&gt;Power Management States: P-States, C-States, and Package C-States&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;Turbo Boost&lt;/h2&gt;
&lt;p&gt;In 2005, Intel introduced &lt;a class="reference external" href="https://en.wikipedia.org/wiki/SpeedStep"&gt;SpeedStep&lt;/a&gt;, a serie of dynamic frequency
scaling technologies to reduce the power consumption of laptop CPUs. Turbo
Boost is an enhancement of these technologies, now also used on desktop and
server CPUs.&lt;/p&gt;
&lt;p&gt;Turbo Boost allows to run one or many CPU cores to higher P-states than usual.
The maximum P-state is constrained by the following factors:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The number of active cores (in C0 or C1 state)&lt;/li&gt;
&lt;li&gt;The estimated current consumption of the processor (Imax)&lt;/li&gt;
&lt;li&gt;The estimated power consumption (TDP - Thermal Design Power) of processor&lt;/li&gt;
&lt;li&gt;The temperature of the processor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example on my laptop:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cat /proc/cpuinfo
model name : Intel(R) Core(TM) i7-3520M CPU &amp;#64; 2.90GHz
...

selma$ sudo cpupower frequency-info
analyzing CPU 0:
  driver: intel_pstate
  ...
  boost state support:
    Supported: yes
    Active: yes
    3400 MHz max turbo 4 active cores
    3400 MHz max turbo 3 active cores
    3400 MHz max turbo 2 active cores
    3600 MHz max turbo 1 active cores
&lt;/pre&gt;
&lt;p&gt;The CPU base frequency is 2.9 GHz. If more than one physical cores is &amp;quot;active&amp;quot;
(busy), their frequency can be increased up to 3.4 GHz. If only 1 physical core
is active, its frequency can be increased up to 3.6 GHz.&lt;/p&gt;
&lt;p&gt;In this example, Turbo Boost is supported and active.&lt;/p&gt;
&lt;p&gt;See also the &lt;a class="reference external" href="https://www.kernel.org/doc/Documentation/cpu-freq/boost.txt"&gt;Linux cpu-freq documentation on CPU boost&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="turbo-boost-msr"&gt;
&lt;h3&gt;Turbo Boost MSR&lt;/h3&gt;
&lt;p&gt;The bit 38 of the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Model-specific_register"&gt;Model-specific register
(MSR)&lt;/a&gt; &lt;tt class="docutils literal"&gt;0x1a0&lt;/tt&gt; can
be used to check if the Turbo Boost is enabled:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ sudo rdmsr -f 38:38 0x1a0
0
&lt;/pre&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;0&lt;/tt&gt; means that Turbo Boost is enabled, whereas &lt;tt class="docutils literal"&gt;1&lt;/tt&gt; means disabled (no
turbo). (The &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-f&lt;/span&gt; 38:38&lt;/tt&gt; option asks to only display the bit 38.)&lt;/p&gt;
&lt;p&gt;If the command doesn't work, you may have to load the &lt;tt class="docutils literal"&gt;msr&lt;/tt&gt; kernel module:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo modprobe msr
&lt;/pre&gt;
&lt;p&gt;Note: I'm not sure that all Intel CPU uses the same MSR.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="intel-state-no-turbo"&gt;
&lt;h3&gt;intel_state/no_turbo&lt;/h3&gt;
&lt;p&gt;Turbo Boost can also be disabled at runtime in the &lt;tt class="docutils literal"&gt;intel_pstate&lt;/tt&gt; driver.&lt;/p&gt;
&lt;p&gt;Check if Turbo Boost is enabled:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cat /sys/devices/system/cpu/intel_pstate/no_turbo
0
&lt;/pre&gt;
&lt;p&gt;where &lt;tt class="docutils literal"&gt;0&lt;/tt&gt; means that Turbo Boost is enabled. Disable Turbo Boost:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ echo 1|sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="cpu-flag-ida"&gt;
&lt;h3&gt;CPU flag &amp;quot;ida&amp;quot;&lt;/h3&gt;
&lt;p&gt;It looks like the Turbo Boost status (supported or not) can also be read by the
CPUID(6): &amp;quot;Thermal/Power Management&amp;quot;. It gives access to the flag &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Intel_Dynamic_Acceleration"&gt;Intel
Dynamic Acceleration (IDA)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;ida&lt;/tt&gt; flag can also be seen in CPU flags of &lt;tt class="docutils literal"&gt;/proc/cpuinfo&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="read-the-cpu-frequency"&gt;
&lt;h2&gt;Read the CPU frequency&lt;/h2&gt;
&lt;p&gt;General information using &lt;tt class="docutils literal"&gt;cpupower &lt;span class="pre"&gt;frequency-info&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cpupower -c 0 frequency-info
analyzing CPU 0:
  driver: intel_pstate
  ...
  hardware limits: 1.20 GHz - 3.60 GHz
  ...
&lt;/pre&gt;
&lt;p&gt;The frequency of CPUs is between 1.2 GHz and 3.6 GHz (the base frequency is
2.9 GHz on this CPU).&lt;/p&gt;
&lt;div class="section" id="get-the-frequency-of-cpus-turbostat"&gt;
&lt;h3&gt;Get the frequency of CPUs: turbostat&lt;/h3&gt;
&lt;p&gt;It looks like the most reliable way to get a relialistic estimation of the CPUs
frequency is to use the tool &lt;tt class="docutils literal"&gt;turbostat&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ sudo turbostat
     CPU Avg_MHz   Busy% Bzy_MHz TSC_MHz
       -     224    7.80    2878    2893
       0     448   15.59    2878    2893
       1       0    0.01    2762    2893
     CPU Avg_MHz   Busy% Bzy_MHz TSC_MHz
       -     139    5.65    2469    2893
       0     278   11.29    2469    2893
       1       0    0.01    2686    2893
    ...
&lt;/pre&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;Avg_MHz&lt;/tt&gt;: average frequency, based on APERF&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;Busy%&lt;/tt&gt;: CPU usage in percent&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;Bzy_MHz&lt;/tt&gt;: busy frequency, based on MPERF&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;TSC_MHz&lt;/tt&gt;: fixed frequency, TSC stands for &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Time_Stamp_Counter"&gt;Time Stamp Counter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;APERF (average) and MPERF (maximum) are MSR registers that can provide feedback
on current CPU frequency.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="other-tools-to-get-the-cpu-frequency"&gt;
&lt;h3&gt;Other tools to get the CPU frequency&lt;/h3&gt;
&lt;p&gt;It looks like the following tools are less reliable to estimate the CPU
frequency.&lt;/p&gt;
&lt;p&gt;cpuinfo:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ grep MHz /proc/cpuinfo
cpu MHz : 1372.289
cpu MHz : 3401.042
&lt;/pre&gt;
&lt;p&gt;In April 2016, Len Brown proposed a patch modifying cpuinfo to use APERF and
MPERF MSR to estimate the CPU frequency: &lt;a class="reference external" href="https://lkml.org/lkml/2016/4/1/7"&gt;x86: Calculate MHz using APERF/MPERF
for cpuinfo and scaling_cur_freq&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;tsc&lt;/tt&gt; clock source logs the CPU frequency in kernel logs:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ dmesg|grep 'MHz processor'
[    0.000000] tsc: Detected 2893.331 MHz processor
&lt;/pre&gt;
&lt;p&gt;cpupower frequency-info:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ for core in $(seq 0 1); do sudo cpupower -c $core frequency-info|grep 'current CPU'; done
  current CPU frequency: 3.48 GHz (asserted by call to hardware)
  current CPU frequency: 3.40 GHz (asserted by call to hardware)
&lt;/pre&gt;
&lt;p&gt;cpupower monitor:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ sudo cpupower monitor -m 'Mperf'
    |Mperf
CPU | C0   | Cx   | Freq
   0|  4.77| 95.23|  1924
   1|  0.01| 99.99|  1751
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Modern Intel CPUs use various technologies to provide best performances without
killing the power consumption. It became harder to monitor and understand CPU
performances, than with older CPUs, since the performance now depends on much
more factors.&lt;/p&gt;
&lt;p&gt;It also becomes common to get an integrated graphics processor (IGP) in the
same package, which makes the exact performance even more complex to predict,
since the IGP produces heat and so has an impact on the CPU P-state.&lt;/p&gt;
&lt;p&gt;I should also explain that P-state are &amp;quot;voted&amp;quot; between CPU cores, but I didn't
understand this part. I'm not sure that understanding the exact algorithm
matters much. I tried to not give too much information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="annex-amt-and-the-me-power-management-coprocessor"&gt;
&lt;h2&gt;Annex: AMT and the ME (power management coprocessor)&lt;/h2&gt;
&lt;p&gt;Computers with Intel vPro technology includes &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Intel_Active_Management_Technology"&gt;Intel Active Management
Technology (AMT)&lt;/a&gt;: &amp;quot;hardware
and firmware technology for remote out-of-band management of personal
computers&amp;quot;. AMT has many features which includes power management.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Intel_Active_Management_Technology#Hardware"&gt;Management Engine (ME)&lt;/a&gt;
is the hardware part: an isolated and protected coprocessor, embedded as a
non-optional part in all current (as of 2015) Intel chipsets. The coprocessor
is a special 32-bit ARC microprocessor (RISC architecture) that's physically
located inside the PCH chipset (or MCH on older chipsets). The coprocessor can
for example be found on Intel MCH chipsets Q35 and Q45.&lt;/p&gt;
&lt;p&gt;See &lt;a class="reference external" href="https://boingboing.net/2016/06/15/intel-x86-processors-ship-with.html"&gt;Intel x86s hide another CPU that can take over your machine (you can't
audit it)&lt;/a&gt; for
more information on the coprocessor.&lt;/p&gt;
&lt;p&gt;More recently, the Intel Xeon Phi CPU (2016) also includes a coprocessor for
power management. I didn't understand if it is the same coprocessor or not.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="optimization"></category><category term="benchmark"></category><category term="cpu"></category></entry><entry><title>Visualize the system noise using perf and CPU isolation</title><link href="https://haypo.github.io/perf-visualize-system-noise-with-cpu-isolation.html" rel="alternate"></link><published>2016-06-16T13:30:00+02:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-06-16:perf-visualize-system-noise-with-cpu-isolation.html</id><summary type="html">&lt;p&gt;I developed a new &lt;a class="reference external" href="http://perf.readthedocs.io/"&gt;perf module&lt;/a&gt; designed to run
stable benchmarks, give fine control on benchmark parameters and compute
statistics on results. With such tool, it becomes simple to &lt;em&gt;visualize&lt;/em&gt;
sources of noise. The CPU isolation will be used to visualize the system noise.
Running a benchmark on isolated CPUs isolates it from the system noise.&lt;/p&gt;
&lt;div class="section" id="isolate-cpus"&gt;
&lt;h2&gt;Isolate CPUs&lt;/h2&gt;
&lt;p&gt;My computer has 4 physical CPU cores. I isolated half of them using
&lt;tt class="docutils literal"&gt;isolcpus=2,3&lt;/tt&gt; parameter of the Linux kernel. I modified manually the command
line in GRUB to add this parameter.&lt;/p&gt;
&lt;p&gt;Check that CPUs are isolated:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/isolated
2-3
&lt;/pre&gt;
&lt;p&gt;The CPU supports HyperThreading, but I disabled it in the BIOS.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="run-a-benchmark"&gt;
&lt;h2&gt;Run a benchmark&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf&lt;/tt&gt; module automatically detects and uses isolated CPU cores. I will
use the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--affinity=0,1&lt;/span&gt;&lt;/tt&gt; option to force running the benchmark on the CPUs
which are not isolated.&lt;/p&gt;
&lt;p&gt;Microbenchmark with and without CPU isolation:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m perf.timeit --json-file=timeit_isolcpus.json --verbose -s 'x=1; y=2' 'x+y'
Pin process to isolated CPUs: 2-3
.........................
Median +- std dev: 36.6 ns +- 0.1 ns (25 runs x 3 samples x 10^7 loops; 1 warmup)

$ python3 -m perf.timeit --affinity=0,1 --json-file=timeit_no_isolcpus.json --verbose -s 'x=1; y=2' 'x+y'
Pin process to CPUs: 0-1
.........................
Median +- std dev: 36.7 ns +- 1.3 ns (25 runs x 3 samples x 10^7 loops; 1 warmup)
&lt;/pre&gt;
&lt;p&gt;My computer was not 100% idle, I was using it while the benchmarks were
running.&lt;/p&gt;
&lt;p&gt;The median is almost the same (36.6 ns and 36.7 ns). The first major difference
is the standard deviation: it is much larger without CPU isolation: 0.1 ns =&amp;gt;
1.3 ns (13x larger).&lt;/p&gt;
&lt;p&gt;Just in case, check manually CPU affinity in metadata:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m perf show timeit_isolcpus.json --metadata | grep cpu
- cpu_affinity: 2-3 (isolated)
- cpu_count: 4
- cpu_model_name: Intel(R) Core(TM) i7-2600 CPU &amp;#64; 3.40GHz

$ python3 -m perf show timeit_no_isolcpus.json --metadata | grep cpu_affinity
- cpu_affinity: 0-1
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="statistics"&gt;
&lt;h2&gt;Statistics&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf stats&lt;/tt&gt; command computes statistics on the distribution of samples:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m perf stats timeit_isolcpus.json
Number of samples: 75

Minimum: 36.5 ns (-0.1%)
Median +- std dev: 36.6 ns +- 0.1 ns (36.5 ns .. 36.7 ns)
Maximum: 36.7 ns (+0.4%)

$ python3 -m perf stats timeit_no_isolcpus.json
Number of samples: 75

Minimum: 36.5 ns (-0.5%)
Median +- std dev: 36.7 ns +- 1.3 ns (35.4 ns .. 38.0 ns)
Maximum: 43.0 ns (+17.0%)
&lt;/pre&gt;
&lt;p&gt;The minimum is the same. The second major difference is the maximum: it is much
larger without CPU isolation: 36.7 ns (+0.4%) =&amp;gt; 43.0 ns (+17.0%).&lt;/p&gt;
&lt;p&gt;The difference between the maximum and the median is 63x larger without CPU
isolation: 0.1 ns (&lt;tt class="docutils literal"&gt;36.7 - 36.6&lt;/tt&gt;) =&amp;gt; 6.3 ns (&lt;tt class="docutils literal"&gt;43.0 - 36.7&lt;/tt&gt;).&lt;/p&gt;
&lt;p&gt;Depending on the system load, a single sample of the microbenchmark is up to
17% slower (maximum of 43.0 ns with a median of 36.7 ns) without CPU isolation.
The difference is smaller with CPU isolation: only 0.4% slower (for the
maximum, and 0.1% faster for the minimum).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="histogram"&gt;
&lt;h2&gt;Histogram&lt;/h2&gt;
&lt;p&gt;Another way to analyze the distribution of samples is to render an histogram:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m perf hist --bins=8 timeit_isolcpus.json timeit_no_isolcpus.json
[ timeit_isolcpus ]
36.1 ns: 75 ################################################
36.9 ns:  0 |
37.7 ns:  0 |
38.5 ns:  0 |
39.3 ns:  0 |
40.1 ns:  0 |
40.9 ns:  0 |
41.7 ns:  0 |
42.5 ns:  0 |

[ timeit_no_isolcpus ]
36.1 ns: 52 ################################################
36.9 ns: 13 ############
37.7 ns:  1 #
38.5 ns:  4 ####
39.3 ns:  2 ##
40.1 ns:  0 |
40.9 ns:  1 #
41.7 ns:  0 |
42.5 ns:  2 ##
&lt;/pre&gt;
&lt;p&gt;I choose the number of bars to get a small histogram and to get all samples of
the first benchmark on the same bar. With 8 bars, each bar is a range of 0.8
ns.&lt;/p&gt;
&lt;p&gt;The last major difference is the shape of these histogram. Without CPU
isolation, there is a &amp;quot;long tail&amp;quot; at the right of the median: &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Outlier"&gt;outliers&lt;/a&gt; in the range [37.7 ns; 42.5 ns].
The outliers come from the &amp;quot;noise&amp;quot; caused by the multitasking system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf&lt;/tt&gt; module provides multiple tools to analyze the distribution of
benchmark samples. Three tools show a major difference without CPU isolation
compared to results with CPU isolation:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Standard deviation: 13x larger without isolation&lt;/li&gt;
&lt;li&gt;Maximum: difference to median 63x larger without isolation&lt;/li&gt;
&lt;li&gt;Shape of the histogram: long tail at the right of the median&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It explains why CPU isolation helps to make benchmarks more stable.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="benchmark"></category></entry><entry><title>My journey to stable benchmark, part 3 (average)</title><link href="https://haypo.github.io/journey-to-stable-benchmark-average.html" rel="alternate"></link><published>2016-05-23T23:00:00+02:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-05-23:journey-to-stable-benchmark-average.html</id><summary type="html">&lt;a class="reference external image-reference" href="https://www.flickr.com/photos/stanzim/11100202065/"&gt;&lt;img alt="Fog" src="https://haypo.github.io/images/fog.jpg" /&gt;&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Stable benchmarks are so close, but ...&lt;/em&gt;&lt;/p&gt;
&lt;div class="section" id="address-space-layout-randomization"&gt;
&lt;h2&gt;Address Space Layout Randomization&lt;/h2&gt;
&lt;p&gt;When I started to work on removing the noise of the system, I was told that
disabling &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Address_space_layout_randomization"&gt;Address Space Layout Randomization (ASLR)&lt;/a&gt; makes
benchmarks more stable.&lt;/p&gt;
&lt;p&gt;I followed this advice without trying to understand it. We will see in this
article that it was a bad idea, but I had to hit other issues to really
understand the root issue with disabling ASLR.&lt;/p&gt;
&lt;p&gt;Example of command to see the effect of ASLR, the first number of the output is
the start address of the heap memory:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python -c 'import os; os.system(&amp;quot;grep heap /proc/%s/maps&amp;quot; % os.getpid())'
55e6a716c000-55e6a7235000 rw-p 00000000 00:00 0                          [heap]
&lt;/pre&gt;
&lt;p&gt;Heap address of 3 runs with ASLR enabled (random):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;55e6a716c000&lt;/li&gt;
&lt;li&gt;561c218eb000&lt;/li&gt;
&lt;li&gt;55e6f628f000&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disable ASLR:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo bash -c 'echo 0 &amp;gt;| /proc/sys/kernel/randomize_va_space'
&lt;/pre&gt;
&lt;p&gt;Heap addresses of 3 runs with ASLR disabled (all the same):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;555555756000&lt;/li&gt;
&lt;li&gt;555555756000&lt;/li&gt;
&lt;li&gt;555555756000&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: To reenable ASLR, it's better to use the value 2, the value 1 only
partially enables the feature:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo bash -c 'echo 2 &amp;gt;| /proc/sys/kernel/randomize_va_space'
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="python-randomized-hash-function"&gt;
&lt;h2&gt;Python randomized hash function&lt;/h2&gt;
&lt;p&gt;With &lt;a class="reference external" href="https://haypo.github.io/journey-to-stable-benchmark-system.html"&gt;system tuning  (part 1)&lt;/a&gt;, a
&lt;a class="reference external" href="https://haypo.github.io/journey-to-stable-benchmark-deadcode.html"&gt;Python compiled with PGO (part 2)&lt;/a&gt;
and ASLR disabled, I still I failed to get the same result when running
manually &lt;tt class="docutils literal"&gt;bm_call_simple.py&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;On Python 3, the hash function is now randomized by default: &lt;a class="reference external" href="http://bugs.python.org/issue13703"&gt;issue #13703&lt;/a&gt;. The problem is that for a
microbenchmark, the number of hash collisions of an &amp;quot;hot&amp;quot; dictionary has a
non-negligible impact on performances.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;PYTHONHASHSEED&lt;/tt&gt; environment variable can be used to get a fixed hash
function. Example with the patch:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ PYTHONHASHSEED=1 taskset -c 1 ./python bm_call_simple.py -n 1
0.198
$ PYTHONHASHSEED=2 taskset -c 1 ./python bm_call_simple.py -n 1
0.201
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1
0.207
$ PYTHONHASHSEED=4 taskset -c 1 ./python bm_call_simple.py -n 1
0.187
$ PYTHONHASHSEED=5 taskset -c 1 ./python bm_call_simple.py -n 1
0.180
&lt;/pre&gt;
&lt;p&gt;Timings of the reference python:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ PYTHONHASHSEED=1 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.204
$ PYTHONHASHSEED=2 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.206
$ PYTHONHASHSEED=3 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.195
$ PYTHONHASHSEED=4 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.192
$ PYTHONHASHSEED=5 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.187
&lt;/pre&gt;
&lt;p&gt;The minimums is 180 ms for the reference and 186 ms for the patch. The patched
Python is 3% faster, yeah!&lt;/p&gt;
&lt;p&gt;Wait. What if we only test PYTHONHASHSEED from 1 to 3? In this case, the
minimum is 195 ms for the reference and 198 ms for the patch. The patched
Python becomes 2% slower, oh no!&lt;/p&gt;
&lt;p&gt;Faster? Slower? Who is right?&lt;/p&gt;
&lt;p&gt;Maybe I should write a script to find a &lt;tt class="docutils literal"&gt;PYTHONHASHSEED&lt;/tt&gt; value for which my
patch is always faster :-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="command-line-and-environment-variables"&gt;
&lt;h2&gt;Command line and environment variables&lt;/h2&gt;
&lt;p&gt;Well, let's say that we will use a fixed PYTHONHASHSEED value. Anyway, my
patch doesn't touch the hash function. So it doesn't matter.&lt;/p&gt;
&lt;p&gt;While running benchmarks, I noticed differences when running the benchmark from
a different directory:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cd /home/haypo/prog/python/fastcall
$ PYTHONHASHSEED=3 taskset -c 1 pgo/python ../benchmarks/performance/bm_call_simple.py -n 1
0.215

$ cd /home/haypo/prog/python/benchmarks
$ PYTHONHASHSEED=3 taskset -c 1 ../fastcall/pgo/python ../benchmarks/performance/bm_call_simple.py -n 1
0.203

$ cd /home/haypo/prog/python
$ PYTHONHASHSEED=3 taskset -c 1 fastcall/pgo/python benchmarks/performance/bm_call_simple.py -n 1
0.200
&lt;/pre&gt;
&lt;p&gt;In fact, a different command line is enough so get different results (added
arguments are ignored):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1
0.201
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1 arg1
0.198
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1 arg1 arg2 arg3
0.203
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1 arg1 arg2 arg3 arg4 arg5
0.206
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1 arg1 arg2 arg3 arg4 arg5 arg6
0.210
&lt;/pre&gt;
&lt;p&gt;I also noticed minor differences when the environment changes (added variables
are ignored):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py -n 1
0.201
$ taskset -c 1 env -i PYTHONHASHSEED=3 VAR1=1 VAR2=2 VAR3=3 VAR4=4 ./python bm_call_simple.py -n 1
0.202
$ taskset -c 1 env -i PYTHONHASHSEED=3 VAR1=1 VAR2=2 VAR3=3 VAR4=4 VAR5=5 ./python bm_call_simple.py -n 1
0.198
&lt;/pre&gt;
&lt;p&gt;Using &lt;tt class="docutils literal"&gt;strace&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;ltrace&lt;/tt&gt;, I saw the memory addresses are different when
something (command line, env var, etc.) changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="average-and-standard-deviation"&gt;
&lt;h2&gt;Average and standard deviation&lt;/h2&gt;
&lt;p&gt;Basically, it looks like a lot of &amp;quot;external factors&amp;quot; have an impact on the
exact memory addresses, even if ASRL is disabled and PYTHONHASHSEED is set. I
started to think how to get &lt;em&gt;exactly&lt;/em&gt; the same command line, the same
environment (easy), the same current directory (easy), etc. The problem is that
it's just not possible to control all external factors (having an effect on the
exact memory addresses).&lt;/p&gt;
&lt;p&gt;Maybe I was plain wrong from the beginning and ASLR must be enabled,
as the default on Linux:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.198
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.202
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.199
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.207
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.200
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.201
&lt;/pre&gt;
&lt;p&gt;These results look &amp;quot;random&amp;quot;. Yes, they are. It's exactly the purpose of ASLR.&lt;/p&gt;
&lt;p&gt;But how can we compare performances if results are random? Take the minimum?&lt;/p&gt;
&lt;p&gt;No! You must never (ever again) use the minimum for benchmarking! Compute the
average and some statistics like the standard deviation:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3
Python 3.4.3
&amp;gt;&amp;gt;&amp;gt; timings=[0.198, 0.202, 0.199, 0.207, 0.200, 0.201]
&amp;gt;&amp;gt;&amp;gt; import statistics
&amp;gt;&amp;gt;&amp;gt; statistics.mean(timings)
0.2011666666666667
&amp;gt;&amp;gt;&amp;gt; statistics.stdev(timings)
0.0031885210782848245
&lt;/pre&gt;
&lt;p&gt;On this example, the average is 201 ms +/- 3 ms. IMHO the standard deviation is
quite small (reliable) which means that my benchmark is stable. To get a good
distribution, it's better to have many samples. It looks like at least 25
processes are needed. Each process tests a different memory layout and a
different hash function.&lt;/p&gt;
&lt;p&gt;Result of 5 runs, each run uses 25 processes (ASLR enabled, random hash
function):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Average: 205.2 ms +/- 3.0 ms (min: 201.1 ms, max: 214.9 ms)&lt;/li&gt;
&lt;li&gt;Average: 205.6 ms +/- 3.3 ms (min: 201.4 ms, max: 216.5 ms)&lt;/li&gt;
&lt;li&gt;Average: 206.0 ms +/- 3.9 ms (min: 201.1 ms, max: 215.3 ms)&lt;/li&gt;
&lt;li&gt;Average: 205.7 ms +/- 3.6 ms (min: 201.5 ms, max: 217.8 ms)&lt;/li&gt;
&lt;li&gt;Average: 206.4 ms +/- 3.5 ms (min: 201.9 ms, max: 214.9 ms)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While memory layout and hash functions are random again, the result looks
&lt;em&gt;less&lt;/em&gt; random, and so more reliable, than before!&lt;/p&gt;
&lt;p&gt;With ASLR enabled, the effect of the environment variables, command line and
current directory is negligible on the (average) result.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-average-solves-issues-with-uniform-random-noises"&gt;
&lt;h2&gt;The average solves issues with uniform random noises&lt;/h2&gt;
&lt;p&gt;The user will run the application with default system settings which means
ASLR enabled and Python hash function randomized. Running a benchmark in one
specific environment is a mistake because it is not representative of the
performance in practice.&lt;/p&gt;
&lt;p&gt;Computing the average and standard deviation &amp;quot;fixes&amp;quot; the issue with hash
randomization. It's much better to use random hash functions and compute the
average, than using a fixed hash function (setting &lt;tt class="docutils literal"&gt;PYTHONHASHSEED&lt;/tt&gt; variable
to a value).&lt;/p&gt;
&lt;p&gt;Oh wow, already 3 big articles explaing how to get stable benchmarks. Please
tell me that it was the last one!  Nope, more is coming...&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="annex-why-only-n1"&gt;
&lt;h2&gt;Annex: why only -n1?&lt;/h2&gt;
&lt;p&gt;In this article, I ran &lt;tt class="docutils literal"&gt;bm_call_simple.py&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-n&lt;/span&gt; 1&lt;/tt&gt; with only run one
iteration.&lt;/p&gt;
&lt;p&gt;Usually, a single iteration is not reliable at all, at least 50 iterations are
needed. But thanks to system tuning, compilation with PGO, ASRL disabled and
&lt;tt class="docutils literal"&gt;PYTHONHASHSEED&lt;/tt&gt; set, a single iteration is enough.&lt;/p&gt;
&lt;p&gt;Example of 3 runs, each with 3 iterations:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py -n 3
0.201
0.201
0.201
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py -n 3
0.201
0.201
0.201
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py -n 3
0.201
0.201
0.201
&lt;/pre&gt;
&lt;p&gt;Always the same timing!&lt;/p&gt;
&lt;/div&gt;
</summary><category term="optimization"></category><category term="benchmark"></category></entry><entry><title>My journey to stable benchmark, part 2 (deadcode)</title><link href="https://haypo.github.io/journey-to-stable-benchmark-deadcode.html" rel="alternate"></link><published>2016-05-22T22:00:00+02:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-05-22:journey-to-stable-benchmark-deadcode.html</id><summary type="html">&lt;a class="reference external image-reference" href="https://www.flickr.com/photos/uw67/16875152403/"&gt;&lt;img alt="Snail" src="https://haypo.github.io/images/snail.jpg" /&gt;&lt;/a&gt;
&lt;p&gt;With &lt;a class="reference external" href="https://haypo.github.io/journey-to-stable-benchmark-system.html"&gt;the system tuning (part 1)&lt;/a&gt;, I
expected to get very stable benchmarks and so I started to benchmark seriously
my &lt;a class="reference external" href="https://bugs.python.org/issue26814"&gt;FASTCALL branch&lt;/a&gt; of CPython (a new
calling convention avoiding temporary tuples).&lt;/p&gt;
&lt;p&gt;I was disappointed to get many slowdowns in the CPython benchmark suite. I
started to analyze why my change introduced performance regressions.&lt;/p&gt;
&lt;p&gt;I took my overall patch and slowly reverted more and more code to check which
changes introduced most of the slowdowns.&lt;/p&gt;
&lt;p&gt;I focused on the &lt;tt class="docutils literal"&gt;call_simple&lt;/tt&gt; benchmark which does only one thing: call
Python functions which do nothing.  Making Python function calls slower would
be a big and inacceptable mistake of my work.&lt;/p&gt;
&lt;div class="section" id="linux-perf"&gt;
&lt;h2&gt;Linux perf&lt;/h2&gt;
&lt;p&gt;I started to learn how to use the great &lt;a class="reference external" href="https://perf.wiki.kernel.org/index.php/Main_Page"&gt;Linux perf&lt;/a&gt; tool to analyze why
&lt;tt class="docutils literal"&gt;call_simple&lt;/tt&gt; was slower. I tried to find a major difference between my
reference python and the patched python.&lt;/p&gt;
&lt;p&gt;I analyzed cache misses on L1 instruction and data caches.  I analyzed stallen
CPU cycles. I analyzed all memory events, branch events, etc. Basically, I tried
all perf events and spent a lot of time to run benchmarks multiple times.&lt;/p&gt;
&lt;p&gt;By the way, I strongly suggest to use &lt;tt class="docutils literal"&gt;perf stat&lt;/tt&gt; using the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--repeat&lt;/span&gt;&lt;/tt&gt;
command line option to get an average on multiple runs and see the standard
deviation. It helps to get more reliable numbers. I even wrote a Python script
implementing &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--repeat&lt;/span&gt;&lt;/tt&gt; (run perf multiple times, parse the output), before
seeing that it was already a builtin feature!&lt;/p&gt;
&lt;p&gt;Use &lt;tt class="docutils literal"&gt;perf list&lt;/tt&gt; to list all available (pre-defined) events.&lt;/p&gt;
&lt;p&gt;After many days, I decided to give up with perf.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cachegrind"&gt;
&lt;h2&gt;Cachegrind&lt;/h2&gt;
&lt;a class="reference external image-reference" href="http://valgrind.org/"&gt;&lt;img alt="Logo of the Valgrind project" src="https://haypo.github.io/images/valgrind.png" /&gt;&lt;/a&gt;
&lt;p&gt;&lt;a class="reference external" href="http://valgrind.org/"&gt;Valgrind&lt;/a&gt; is a great tool known to detect memory
leaks, but it also contains gems like the &lt;a class="reference external" href="http://valgrind.org/docs/manual/cg-manual.html"&gt;Cachegrind tool&lt;/a&gt; which &lt;em&gt;simulates&lt;/em&gt; the
CPU caches.&lt;/p&gt;
&lt;p&gt;I used Cachegrind with the nice &lt;a class="reference external" href="http://kcachegrind.sourceforge.net/"&gt;Kcachegrind GUI&lt;/a&gt;. Sadly, I also failed to see anything
obvious in cache misses between the reference python and the patched python.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="strace-and-ltrace"&gt;
&lt;h2&gt;strace and ltrace&lt;/h2&gt;
&lt;img alt="strace and ltrace" src="https://haypo.github.io/images/strace_ltrace.png" /&gt;
&lt;p&gt;I also tried &lt;tt class="docutils literal"&gt;strace&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;ltrace&lt;/tt&gt; tools to try to see a difference in the
execution of the reference and the patched pythons. I saw different memory
addresses, but no major difference which can explain a difference of the
timing.&lt;/p&gt;
&lt;p&gt;Morever, the hotcode simply does not call any syscall nor library
function. It's pure CPU-bound code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="compiler-options"&gt;
&lt;h2&gt;Compiler options&lt;/h2&gt;
&lt;a class="reference external image-reference" href="https://gcc.gnu.org/"&gt;&lt;img alt="GCC logo" class="align-right" src="https://haypo.github.io/images/gcc.png" /&gt;&lt;/a&gt;
&lt;p&gt;I used &lt;a class="reference external" href="https://gcc.gnu.org/"&gt;GCC&lt;/a&gt; to build to code. Just in case, I tried
LLVM compiler, but it didn't &amp;quot;fix&amp;quot; the issue.&lt;/p&gt;
&lt;p&gt;I also tried different optimization levels: &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O0&lt;/span&gt;&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O1&lt;/span&gt;&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O2&lt;/span&gt;&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O3&lt;/span&gt;&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;I read that the exact address of functions can have an impact on the CPU L1
cache: &lt;a class="reference external" href="https://stackoverflow.com/questions/19470873/why-does-gcc-generate-15-20-faster-code-if-i-optimize-for-size-instead-of-speed"&gt;Why does gcc generate 15-20% faster code if I optimize for size instead
of speed?&lt;/a&gt;.
I tried various values of the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-falign-functions=N&lt;/span&gt;&lt;/tt&gt; option (1, 2, 6, 12).&lt;/p&gt;
&lt;p&gt;I also tried &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fomit-pointer&lt;/span&gt;&lt;/tt&gt; (omit frame pointer) to record the callgraph with &lt;tt class="docutils literal"&gt;perf record&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;I also tried &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-flto&lt;/span&gt;&lt;/tt&gt;: Link Time Optimization (LTO).&lt;/p&gt;
&lt;p&gt;These compiler options didn't fix the issue.&lt;/p&gt;
&lt;p&gt;The truth is out there.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; See also &lt;a class="reference external" href="https://lwn.net/Articles/534735/"&gt;Rethinking optimization for size&lt;/a&gt; article on Linux Weekly News (LWN):
&lt;em&gt;&amp;quot;Such an option has obvious value if one is compiling for a
space-constrained environment like a small device. But it turns out that, in
some situations, optimizing for space can also produce faster code.&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="when-cpython-performance-depends-on-dead-code"&gt;
&lt;h2&gt;When CPython performance depends on dead code&lt;/h2&gt;
&lt;p&gt;I continued to revert changes. At the end, my giant patch was reduced to very
few changes only adding code which was never called (at least, I was sure
that it was not called in the &lt;tt class="docutils literal"&gt;call_simple&lt;/tt&gt; benchmark).&lt;/p&gt;
&lt;p&gt;Let me rephase: &lt;em&gt;adding dead code&lt;/em&gt; makes Python slower. What?&lt;/p&gt;
&lt;p&gt;A colleague suggested me to remove the body (replace it with &lt;tt class="docutils literal"&gt;return;&lt;/tt&gt;) of
added function: the code became faster. Ok, now I'm completely lost. To be
clear, I don't expect that adding dead code would have &lt;em&gt;any&lt;/em&gt; impact on the
performance.&lt;/p&gt;
&lt;p&gt;My email &lt;a class="reference external" href="https://mail.python.org/pipermail/speed/2016-April/000341.html"&gt;When CPython performance depends on dead code...&lt;/a&gt; explains how
to reproduce the issue and contains many information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="solution-pgo"&gt;
&lt;h2&gt;Solution: PGO&lt;/h2&gt;
&lt;p&gt;The solution is called Profiled Guided Optimization, &amp;quot;PGO&amp;quot;. Python build system
supports it in a single command: &lt;tt class="docutils literal"&gt;make &lt;span class="pre"&gt;profile-opt&lt;/span&gt;&lt;/tt&gt;. It profiles the
execution of the Python test suite.&lt;/p&gt;
&lt;p&gt;Using PGO, adding dead code has no more impact on the performance.&lt;/p&gt;
&lt;p&gt;With system tuning and PGO compilation, benchmarks must now be stable this
time, no? ... No, sorry, not yet. We will see more sources of noise in
following articles ;-)&lt;/p&gt;
&lt;/div&gt;
</summary><category term="optimization"></category><category term="benchmark"></category></entry><entry><title>My journey to stable benchmark, part 1 (system)</title><link href="https://haypo.github.io/journey-to-stable-benchmark-system.html" rel="alternate"></link><published>2016-05-21T16:50:00+02:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-05-21:journey-to-stable-benchmark-system.html</id><summary type="html">&lt;div class="section" id="background"&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;In the CPython development, it became common to require the result of the
&lt;a class="reference external" href="https://hg.python.org/benchmarks"&gt;CPython benchmark suite&lt;/a&gt; (&amp;quot;The Grand
Unified Python Benchmark Suite&amp;quot;) to evaluate the effect of an optimization
patch. The minimum requirement is to not introduce performance regressions.&lt;/p&gt;
&lt;p&gt;I used the CPython benchmark suite and I had many bad surprises when trying to
analyze (understand) results. A change expected to be faster makes some
benchmarks slower without any obvious reason. At least, the change is expected
to be faster on some specific benchmarks, but have no impact on the other
benchmarks. The slowdown is usually between 5% and 10% slower. I am not
confortable with any kind of slowdown.&lt;/p&gt;
&lt;p&gt;Many benchmarks look unstable. The problem is to trust the overall report.
Some developers started to say that they learnt to ignore some benchmarks known
to be unstable.&lt;/p&gt;
&lt;p&gt;It's not the first time that I am totally disappointed by microbenchmark
results, so I decided to analyze completely the issue and go as deep as
possible to really understand the problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-to-get-stable-benchmarks-on-a-busy-linux-system"&gt;
&lt;h2&gt;How to get stable benchmarks on a busy Linux system&lt;/h2&gt;
&lt;p&gt;A common advice to get stable benchmark is to stay away the keyboard
(&amp;quot;freeze!&amp;quot;) and stop all other applications to only run one application, the
benchmark.&lt;/p&gt;
&lt;p&gt;Well, I'm working on a single computer and the full CPython benchmark suite
take up to 2 hours in rigorous mode. I just cannot stop working during 2 hours
to wait for the result of the benchmark. I like running benchmarks locally. It
is convenient to run benchmarks on the same computer used to develop.&lt;/p&gt;
&lt;p&gt;The goal here is to &amp;quot;remove the noise of the system&amp;quot;. Get the same result on a
busy system than an idle system. My simple &lt;a class="reference external" href="https://bitbucket.org/haypo/misc/src/tip/bin/system_load.py"&gt;system_load.py&lt;/a&gt; program can be
used to increase the system load. For example, run &lt;tt class="docutils literal"&gt;system_load.py 10&lt;/tt&gt; in a
terminal to get at least a system load of 10 (busy system) and run the
benchmark in a different terminal. Use CTRL+c to stop &lt;tt class="docutils literal"&gt;system_load.py&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cpu-isolation"&gt;
&lt;h2&gt;CPU isolation&lt;/h2&gt;
&lt;p&gt;In 2016, it is common to get a CPU with multiple physical cores. For example,
my Intel CPU has 4 physical cores and 8 logical cores thanks to
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Hyper-threading"&gt;Hyper-Threading&lt;/a&gt;. It is
possible to configure the Linux kernel to not schedule processes on some CPUs
using the &amp;quot;CPU isolation&amp;quot; feature. It is the &lt;tt class="docutils literal"&gt;isolcpus&lt;/tt&gt; parameter of the
Linux command line, the value is a list of CPUs. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
isolcpus=2,3,6,7
&lt;/pre&gt;
&lt;p&gt;Check with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/isolated
2-3,6-7
&lt;/pre&gt;
&lt;p&gt;If you have Hyper-Threading, you must isolate the two logicial cores of each
isolated physical core. You can use the &lt;tt class="docutils literal"&gt;lscpu &lt;span class="pre"&gt;--all&lt;/span&gt; &lt;span class="pre"&gt;--extended&lt;/span&gt;&lt;/tt&gt; command to
identify physical cores. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ lscpu -a -e
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE MAXMHZ    MINMHZ
0   0    0      0    0:0:0:0       yes    5900,0000 1600,0000
1   0    0      1    1:1:1:0       yes    5900,0000 1600,0000
2   0    0      2    2:2:2:0       yes    5900,0000 1600,0000
3   0    0      3    3:3:3:0       yes    5900,0000 1600,0000
4   0    0      0    0:0:0:0       yes    5900,0000 1600,0000
5   0    0      1    1:1:1:0       yes    5900,0000 1600,0000
6   0    0      2    2:2:2:0       yes    5900,0000 1600,0000
7   0    0      3    3:3:3:0       yes    5900,0000 1600,0000
&lt;/pre&gt;
&lt;p&gt;The physical core &lt;tt class="docutils literal"&gt;0&lt;/tt&gt; (CORE column) is made of two logical cores (CPU
column): &lt;tt class="docutils literal"&gt;0&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;4&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="nohz-mode"&gt;
&lt;h2&gt;NOHZ mode&lt;/h2&gt;
&lt;p&gt;By default, the Linux kernel uses a scheduling-clock which interrupts the
running application &lt;tt class="docutils literal"&gt;HZ&lt;/tt&gt; times per second to run the scheduler. &lt;tt class="docutils literal"&gt;HZ&lt;/tt&gt; is
usually between 100 and 1000: time slice between 1 ms and 10 ms.&lt;/p&gt;
&lt;p&gt;Linux supports a &lt;a class="reference external" href="https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt"&gt;NOHZ mode&lt;/a&gt; which is able to
disable the scheduling-clock when the system is idle to reduce the power
consumption. Linux 3.10 introduces a &lt;a class="reference external" href="https://lwn.net/Articles/549580/"&gt;full ticketless mode&lt;/a&gt;, NOHZ full, which is able to disable the
scheduling-clock when only one application is running on a CPU.&lt;/p&gt;
&lt;p&gt;NOHZ full is disabled by default. It can be enabled with the &lt;tt class="docutils literal"&gt;nohz_full&lt;/tt&gt;
parameter of the Linux command line, the value is a list of CPUs. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
nohz_full=2,3,6,7
&lt;/pre&gt;
&lt;p&gt;Check with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/nohz_full
2-3,6-7
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="interrupts-irq"&gt;
&lt;h2&gt;Interrupts (IRQ)&lt;/h2&gt;
&lt;p&gt;The Linux kernel can also be configured to not run &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Interrupt_request_%28PC_architecture%29"&gt;interruptions (IRQ)&lt;/a&gt;
handlers on some CPUs using &lt;tt class="docutils literal"&gt;/proc/irq/default_smp_affinity&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;/proc/irq/&amp;lt;number&amp;gt;/smp_affinity&lt;/span&gt;&lt;/tt&gt; files. The value is not a list of CPUs but
a bitmask.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;/proc/interrupts&lt;/tt&gt; file can be read to see the number of interruptions
per CPU.&lt;/p&gt;
&lt;p&gt;Read the &lt;a class="reference external" href="https://www.kernel.org/doc/Documentation/IRQ-affinity.txt"&gt;Linux SMP IRQ affinity&lt;/a&gt; documentation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example-of-effect-of-cpu-isolation-on-a-microbenchmark"&gt;
&lt;h2&gt;Example of effect of CPU isolation on a microbenchmark&lt;/h2&gt;
&lt;p&gt;Example with Linux parameters:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
isolcpus=2,3,6,7 nohz_full=2,3,6,7
&lt;/pre&gt;
&lt;p&gt;Microbenchmark on an idle system (without CPU isolation):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m timeit 'sum(range(10**7))'
10 loops, best of 3: 229 msec per loop
&lt;/pre&gt;
&lt;p&gt;Result on a busy system using &lt;tt class="docutils literal"&gt;system_load.py 10&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;find /&lt;/tt&gt; commands
running in other terminals:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m timeit 'sum(range(10**7))'
10 loops, best of 3: 372 msec per loop
&lt;/pre&gt;
&lt;p&gt;The microbenchmark is 56% slower because of the high system load!&lt;/p&gt;
&lt;p&gt;Result on the same busy system but using isolated CPUs. The &lt;tt class="docutils literal"&gt;taskset&lt;/tt&gt; command
allows to pin an application to specific CPUs:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ taskset -c 1,3 python3 -m timeit 'sum(range(10**7))'
10 loops, best of 3: 230 msec per loop
&lt;/pre&gt;
&lt;p&gt;Just to check, new run without CPU isolation:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m timeit 'sum(range(10**7))'
10 loops, best of 3: 357 msec per loop
&lt;/pre&gt;
&lt;p&gt;The result with CPU isolation on a busy system is the same than the result an
idle system! CPU isolation removes most of the noise of the system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Great job Linux!&lt;/p&gt;
&lt;p&gt;Ok! Now, the benchmark is super stable, no? ...  Sorry, no, it's not stable yet.
I found a lot of other sources of &amp;quot;noise&amp;quot;.  We will see them in the following
articles ;-)&lt;/p&gt;
&lt;/div&gt;
</summary><category term="optimization"></category><category term="benchmark"></category></entry><entry><title>Status of Python 3 in OpenStack Mitaka</title><link href="https://haypo.github.io/openstack_mitaka_python3.html" rel="alternate"></link><published>2016-03-02T14:00:00+01:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-03-02:openstack_mitaka_python3.html</id><summary type="html">&lt;p&gt;Now that most OpenStack services have reached feature freeze for the Mitaka
cycle (November 2015-April 2016), it's time to look back on the progress made
for Python 3 support.&lt;/p&gt;
&lt;p&gt;Previous status update: &lt;a class="reference external" href="http://techs.enovance.com/7807/python-3-status-openstack-liberty"&gt;Python 3 Status in OpenStack Liberty&lt;/a&gt;
(September 2015).&lt;/p&gt;
&lt;div class="section" id="services-ported-to-python-3"&gt;
&lt;h2&gt;Services ported to Python 3&lt;/h2&gt;
&lt;p&gt;13 services were ported to Python 3 during the Mitaka cycle:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Cinder&lt;/li&gt;
&lt;li&gt;Congress&lt;/li&gt;
&lt;li&gt;Designate&lt;/li&gt;
&lt;li&gt;Glance&lt;/li&gt;
&lt;li&gt;Heat&lt;/li&gt;
&lt;li&gt;Horizon&lt;/li&gt;
&lt;li&gt;Manila&lt;/li&gt;
&lt;li&gt;Mistral&lt;/li&gt;
&lt;li&gt;Octavia&lt;/li&gt;
&lt;li&gt;Searchlight&lt;/li&gt;
&lt;li&gt;Solum&lt;/li&gt;
&lt;li&gt;Watcher&lt;/li&gt;
&lt;li&gt;Zaqar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Red Hat contributed to the Cinder, Designate, Glance and Horizon service
porting efforts.&lt;/p&gt;
&lt;p&gt;&amp;quot;Ported to Python 3&amp;quot; means that all unit tests pass on Python 3.4 which is
verified by a voting gate job. It is not enough to run applications in
production with Python 3. Integration and functional tests are not run on
Python 3 yet. See the section dedicated to these tests below.&lt;/p&gt;
&lt;p&gt;See the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Python3"&gt;Python 3 wiki page&lt;/a&gt; for the
current status of the OpenStack port to Python 3; especially the list of
services ported to Python 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="services-not-ported-yet"&gt;
&lt;h2&gt;Services not ported yet&lt;/h2&gt;
&lt;p&gt;It's become easier to list services which are not compatible with Python 3 than
listing services already ported to Python 3!&lt;/p&gt;
&lt;p&gt;9 services still need to be ported:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Work-in-progress:&lt;ul&gt;
&lt;li&gt;Magnum: 83% (959 unit tests/1,161)&lt;/li&gt;
&lt;li&gt;Cue: 81% (208 unit tests/257)&lt;/li&gt;
&lt;li&gt;Nova: 74% (10,859 unit tests/14,726)&lt;/li&gt;
&lt;li&gt;Barbican: 34% (392 unit tests/1168)&lt;/li&gt;
&lt;li&gt;Murano: 29% (133 unit tests/455)&lt;/li&gt;
&lt;li&gt;Keystone: 27% (1200 unit tests/4455)&lt;/li&gt;
&lt;li&gt;Swift: 0% (3 unit tests/4,435)&lt;/li&gt;
&lt;li&gt;Neutron-LBaaS: 0% (1 unit test/806)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Port not started yet:&lt;ul&gt;
&lt;li&gt;Trove: no python34 gate&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Red Hat contributed Python 3 patches to Cue, Neutron-LBaaS, Swift and Trove
during the Mitaka cycle.&lt;/p&gt;
&lt;p&gt;Trove developers are ready to start the port at the beginning of the next cycle
(Newton). The py34 test environment was blocked by the MySQL-Python dependency (it
was not possible to build the test environment), but this dependency is now
skipped on Python 3. Later, it will be &lt;a class="reference external" href="https://review.openstack.org/#/c/225915/"&gt;replaced with PyMySQL&lt;/a&gt; on Python 2 and Python 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-3-issues-in-eventlet"&gt;
&lt;h2&gt;Python 3 issues in Eventlet&lt;/h2&gt;
&lt;p&gt;Four Python 3 issues were fixed in Eventlet:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/eventlet/eventlet/issues/295"&gt;Issue #295: Python 3: wsgi doesn't handle correctly partial write of
socket send() when using writelines()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PR #275: &lt;a class="reference external" href="https://github.com/eventlet/eventlet/pull/275"&gt;Issue #274: Fix GreenSocket.recv_into()&lt;/a&gt;.
Issue: &lt;a class="reference external" href="https://github.com/eventlet/eventlet/issues/274"&gt;On Python 3, sock.makefile('rb').readline() doesn't handle blocking
errors correctly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PR #257: &lt;a class="reference external" href="https://github.com/eventlet/eventlet/pull/257"&gt;Fix GreenFileIO.readall() for regular file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/eventlet/eventlet/issues/248"&gt;Issue #248: eventlet.monkey_patch() on Python 3.4 makes stdout
non-blocking&lt;/a&gt;: pull
request &lt;a class="reference external" href="https://github.com/eventlet/eventlet/pull/250"&gt;Fix GreenFileIO.write()&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="next-milestone-functional-and-integration-tests"&gt;
&lt;h2&gt;Next Milestone: Functional and integration tests&lt;/h2&gt;
&lt;p&gt;The next major milestone will be to run functional and integration tests on
Python 3.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;functional tests are restricted to one component (ex: only Glance)&lt;/li&gt;
&lt;li&gt;integration tests, like Tempest, test the integration of multiple components&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is now possible to install some packages on Python 3 in DevStack using
&lt;tt class="docutils literal"&gt;USE_PYTHON3&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;PYTHON3_VERSION&lt;/tt&gt; variables: &lt;a class="reference external" href="https://review.openstack.org/#/c/181165/"&gt;Enable optional Python 3
support&lt;/a&gt;. It means that it is
possible to run tests with some services running on Python 3, and the remaining
services on Python 2.&lt;/p&gt;
&lt;p&gt;The port to Python 3 of Glance, Heat and Neutron functional and integration
tests have already started.&lt;/p&gt;
&lt;p&gt;For Glance, 159 functional tests already pass on Python 3.4.&lt;/p&gt;
&lt;p&gt;Heat:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;project-config: &lt;a class="reference external" href="https://review.openstack.org/#/c/228194/"&gt;Add python34 integration test job for Heat&lt;/a&gt; (WIP)&lt;/li&gt;
&lt;li&gt;heat: &lt;a class="reference external" href="https://review.openstack.org/#/c/188033/"&gt;py34: integration tests&lt;/a&gt;
(WIP)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Neutron: the &lt;a class="reference external" href="https://review.openstack.org/#/c/231897/"&gt;Add the functional-py34 and dsvm-functional-py34 targets to
tox.ini&lt;/a&gt; change was merged, but a
gate job hasn't been added for it yet.&lt;/p&gt;
&lt;p&gt;Another pending project is to fix issues specific to Python 3.5, but the gate
doesn’t use Python 3.5 yet. There are some minor issues, probably easy to fix.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-to-port-remaining-code"&gt;
&lt;h2&gt;How to port remaining code?&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Python3"&gt;Python 3 wiki page&lt;/a&gt; contains
a lot of information about adding Python 3 support to Python 2 code.&lt;/p&gt;
&lt;p&gt;Join us in the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;#openstack-python3&lt;/span&gt;&lt;/tt&gt; IRC channel on Freenode to discuss
Python 3!&lt;/p&gt;
&lt;/div&gt;
</summary><category term="openstack"></category><category term="python3"></category></entry><entry><title>Fast _PyAccu, _PyUnicodeWriter and_PyBytesWriter APIs to produce strings in CPython</title><link href="https://haypo.github.io/pybyteswriter.html" rel="alternate"></link><published>2016-03-01T16:00:00+01:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-03-01:pybyteswriter.html</id><summary type="html">&lt;p&gt;This article described the _PyBytesWriter and _PyUnicodeWriter private APIs of
CPython. These APIs are design to optimize code producing strings when the
ouput size is not known in advance.&lt;/p&gt;
&lt;p&gt;I created the _PyUnicodeWriter API to reply to complains that Python 3 was much
slower than Python 2, especially with the new Unicode implementation (PEP 393).&lt;/p&gt;
&lt;div class="section" id="pyaccu-api"&gt;
&lt;h2&gt;_PyAccu API&lt;/h2&gt;
&lt;p&gt;Issue #12778: In 2011, Antoine Pitrou found a performance issue in the JSON
serializer when serializing many small objects: it used way too much memory for
temporary objects compared to the final output string.&lt;/p&gt;
&lt;p&gt;The JSON serializer used a list of strings and joined all strings at the end of
create a final output string. Pseudocode:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def serialize():
    pieces = [serialize(item) for item in self]
    return ''.join(pieces)
&lt;/pre&gt;
&lt;p&gt;Antoine introduced an accumulator compacting the temporary list of &amp;quot;small&amp;quot;
strings and put the result in a second list of &amp;quot;large&amp;quot; strings. At the end, the
list of &amp;quot;large&amp;quot; strings was also compacted to build the final output string.
Pseudo-code:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def serialize():
    small = []
    large = []
    for item in self:
        small.append(serialize(item))
        if len(small) &amp;gt; 10000:
            large.append(''.join(small))
            small.clear()
    if small
        large.append(''.join(small))
    return ''.join(large)
&lt;/pre&gt;
&lt;p&gt;The threshold of 10,000  strings is justified by this comment:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
/* Each item in a list of unicode objects has an overhead (in 64-bit
 * builds) of:
 *   - 8 bytes for the list slot
 *   - 56 bytes for the header of the unicode object
 * that is, 64 bytes.  100000 such objects waste more than 6MB
 * compared to a single concatenated string.
 */
&lt;/pre&gt;
&lt;p&gt;Issue #12911: Antoine Pitrou found a similar performance issue in repr(list),
and so proposed to convert its accumular code into a new private _PyAccu API.
He added the _PyAccu API to Python 2.7.5 and 3.2.3. Title of te repr(list)
change: &amp;quot;Fix memory consumption when calculating the repr() of huge tuples or
lists&amp;quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-pyunicodewriter-api"&gt;
&lt;h2&gt;The _PyUnicodeWriter API&lt;/h2&gt;
&lt;div class="section" id="inefficient-implementation-of-the-pep-393"&gt;
&lt;h3&gt;Inefficient implementation of the PEP 393&lt;/h3&gt;
&lt;p&gt;In 2010, Python 3.3 got a completly new Unicode implementation, the Python type
&lt;tt class="docutils literal"&gt;str&lt;/tt&gt;, with the PEP 393. The implementation of the PEP was the topic of a
Google Summer of Code 2011 with the student Torsten Becker menthored by Martin
v. Löwis (author of the PEP). The project was successful: the PEP 393 was
implemented, it worked!&lt;/p&gt;
&lt;p&gt;The first implementation of the PEP 393 used a lot of 32-bit character buffers
(&lt;tt class="docutils literal"&gt;Py_UCS4&lt;/tt&gt;) which uses a lot of memory and requires expensive conversion to
8-bit (&lt;tt class="docutils literal"&gt;Py_UCS1&lt;/tt&gt;, ASCII and Latin1) or 16-bit (&lt;tt class="docutils literal"&gt;Py_UCS2&lt;/tt&gt;, BMP) characters.&lt;/p&gt;
&lt;p&gt;The new internal structures for Unicode strings are now very complex and
require to be smart when building a new string to avoid memory copies. I
created the _PyUnicodeWriter API to try to reduce expensive memory copies, and
even completly avoid memory copies in best cases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="design-of-the-pyunicodewriter-api"&gt;
&lt;h3&gt;Design of the _PyUnicodeWriter API&lt;/h3&gt;
&lt;p&gt;According to benchmarks, creating a &lt;tt class="docutils literal"&gt;Py_UCS1*&lt;/tt&gt; buffer and then expand it
to &lt;tt class="docutils literal"&gt;Py_UCS2*&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;Py_UCS4*&lt;/tt&gt; is more efficient, since &lt;tt class="docutils literal"&gt;Py_UCS1*&lt;/tt&gt; is the
most common format.&lt;/p&gt;
&lt;p&gt;Python &lt;tt class="docutils literal"&gt;str&lt;/tt&gt; type is used for a wide range of usages. For example, it is used
for the name of variable names in the Python language itself. Variable names
are almost always ASCII.&lt;/p&gt;
&lt;p&gt;The worst case for _PyUnicodeWriter is when a long &lt;tt class="docutils literal"&gt;Py_UCS1*&lt;/tt&gt; buffer must be
converted to &lt;tt class="docutils literal"&gt;Py_UCS2*&lt;/tt&gt;, and then converted to &lt;tt class="docutils literal"&gt;Py_UCS4*&lt;/tt&gt;. Each conversion
is expensive: need to allocate a second memory block and convert characters to
the new format.&lt;/p&gt;
&lt;p&gt;_PyUnicodeWriter features:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Optional overallocation: overallocate the buffer by 50% on Windows and 25%
on Linux. The ratio changes depending on the OS, it is a raw heuristic to get
the best performances depending on the &lt;tt class="docutils literal"&gt;malloc()&lt;/tt&gt; memory allocator.&lt;/li&gt;
&lt;li&gt;The buffer can be a shared read-only string if the buffer was only created
from a single string. Micro-optimization for &lt;tt class="docutils literal"&gt;&amp;quot;%s&amp;quot; % str&lt;/tt&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The API allows to disable overallocation before the last write. For example,
&lt;tt class="docutils literal"&gt;&amp;quot;%s%s&amp;quot; % ('abc', 'def')&lt;/tt&gt; disables the overallocation before writing
&lt;tt class="docutils literal"&gt;'def'&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;The _PyUnicodeWriter was introduced by the issue #14716 (change 7be716a47e9d):&lt;/p&gt;
&lt;blockquote&gt;
Close #14716: str.format() now uses the new &amp;quot;unicode writer&amp;quot; API instead
of the PyAccu API. For example, it makes str.format() from 25% to 30%
faster on Linux.&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="section" id="fast-path-for-ascii"&gt;
&lt;h3&gt;Fast-path for ASCII&lt;/h3&gt;
&lt;p&gt;The cool and &lt;em&gt;unexpected&lt;/em&gt; side-effect of the _PyUnicodeWriter is that many
intermediate operations got a fast-path for &lt;tt class="docutils literal"&gt;Py_UCS1*&lt;/tt&gt;, especially for ASCII
strings. For example, padding a number with spaces on &lt;tt class="docutils literal"&gt;'%10i' % 123&lt;/tt&gt; is
implemented with &lt;tt class="docutils literal"&gt;memset()&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;Formating a floating point number uses the &lt;tt class="docutils literal"&gt;PyOS_double_to_string()&lt;/tt&gt; function
which creates an ASCII buffer. If the writer buffer uses Py_UCS1, a
&lt;tt class="docutils literal"&gt;memcpy()&lt;/tt&gt; is enough to copy the formatted number.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="avoid-temporary-buffers"&gt;
&lt;h3&gt;Avoid temporary buffers&lt;/h3&gt;
&lt;p&gt;Since the beginning, I had the idea of avoiding temporary buffers thanks
to an unified API to handle a &amp;quot;Unicode buffer&amp;quot;. Slowly, I spread my changes
to all functions producing Unicode strings.&lt;/p&gt;
&lt;p&gt;The obvious target were &lt;tt class="docutils literal"&gt;str % args&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;str.format(args)&lt;/tt&gt;. Both
instructions use very different code, but it was possible to share a few
functions especially the code to format integers in bases 2 (binary), 8
(octal), 10 (decimal) and 16 (hexadecimal).&lt;/p&gt;
&lt;p&gt;The function formatting an integer computes the exact size of the output,
requests a number of characters and then write characters. The characters are
written directly in the writer buffer. No temporary memory block is needed
anymore, and moreover no Py_UCS conversion is need: &lt;tt class="docutils literal"&gt;_PyLong_Format()&lt;/tt&gt; writes
directly characters into the character format (PyUCS1, Py_UCS2 or Py_UCS4) of
the buffer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="performance-compared-to-python-2"&gt;
&lt;h3&gt;Performance compared to Python 2&lt;/h3&gt;
&lt;p&gt;The PEP 393 uses a complex storage for strings, so the exact performances
now depends on the character set used in the benchmark. For benchmarks using
a character set different than ASCII, the result are more tricky to understand.&lt;/p&gt;
&lt;p&gt;To compare performances with Python 2, I focused my benchmarks on ASCII.  I
compared Python 3 str with Python 2 unicode, but also sometimes to Python 2 str
(bytes). On ASCII, Python 3.3 was as fast as Python 2, or even faster on some
very specific cases, but these cases are probably artificial and never seen in
real applications.&lt;/p&gt;
&lt;p&gt;In the best case, Python 3 str (Unicode) was faster than Python 2 bytes.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="pybyteswriter-api-first-try-big-fail"&gt;
&lt;h2&gt;_PyBytesWriter API: first try, big fail&lt;/h2&gt;
&lt;p&gt;Since Python was &lt;em&gt;much&lt;/em&gt; faster with _PyUnicodeWriter, I expected to get good
speedup with a similar API for bytes. The graal would be to share code for
bytes and Unicode (Spoiler alert! I reached this goal, but only for a single
function: format an integer to decimal).&lt;/p&gt;
&lt;p&gt;My first attempt of a _PyBytesWriter API was in 2013: &lt;a class="reference external" href="https://bugs.python.org/issue17742"&gt;Issue #17742: Add
_PyBytesWriter API&lt;/a&gt;. But quickly, I
noticed with microbenchmarks that my change made Python slower! I spent hours
to understand why GCC produced less efficient machine code. When I started to
dig the &amp;quot;strict aliasing&amp;quot; optimization issue, I realized that I reached a
deadend.&lt;/p&gt;
&lt;p&gt;Extract of the _PyBytesWriter structure:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
typedef struct {
    /* Current position in the buffer */
    char *str;

    /* Start of the buffer */
    char *start;

    /* End of the buffer */
    char *end;

    ...
} _PyBytesWriter;
&lt;/pre&gt;
&lt;p&gt;The problem is that GCC emited less efficient machine code for the C code (see
my &lt;a class="reference external" href="https://bugs.python.org/issue17742#msg187595"&gt;msg187595&lt;/a&gt;):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
while (collstart++&amp;lt;collend)
    *writer.str++ = '?';
&lt;/pre&gt;
&lt;p&gt;For the &lt;tt class="docutils literal"&gt;writer.str++&lt;/tt&gt; instruction, the new pointer value is written
immediatly in the structure. The pointer value is read again at each iteration.
So we have 1 LOAD and 1 STORE per iteration.&lt;/p&gt;
&lt;p&gt;GCC emits better code for the original C code:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
while (collstart++&amp;lt;collend)
    *str++ = '?';
&lt;/pre&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;str&lt;/tt&gt; variable is stored in a register and the new value of &lt;tt class="docutils literal"&gt;str&lt;/tt&gt; is
only written &lt;em&gt;once&lt;/em&gt;, at the end of loop (instead of writing it at each
iteration). The pointer value is &lt;em&gt;only read once&lt;/em&gt; before the loop. So we have 0
LOAD and 0 STORE (related to the pointer value) in the loop body.&lt;/p&gt;
&lt;p&gt;It looks like an aliasing issue, but I didn't find how to say to GCC that the
new value of &lt;tt class="docutils literal"&gt;writer.str&lt;/tt&gt; can be written only once at the end of the loop. I
tried the &lt;tt class="docutils literal"&gt;__restrict__&lt;/tt&gt; keyword: the LOAD (get the pointer value) was moved
out of the loop. But the STORE was still in the loop body.&lt;/p&gt;
&lt;p&gt;I wrote to gcc-help: &lt;a class="reference external" href="https://gcc.gnu.org/ml/gcc-help/2013-04/msg00192.html"&gt;Missed optimization when using a structure&lt;/a&gt;, but I didn't get any
reply. I just gave up.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="pybyteswriter-api-new-try-the-good-one"&gt;
&lt;h2&gt;_PyBytesWriter API: new try, the good one&lt;/h2&gt;
&lt;p&gt;In 2015, I created the &lt;a class="reference external" href="https://bugs.python.org/issue25318"&gt;Issue #25318: Add _PyBytesWriter API to optimize
Unicode encoders&lt;/a&gt;. I redesigned the API
to avoid the aliasing issue.&lt;/p&gt;
&lt;p&gt;The new _PyBytesWriter doesn't contain the &lt;tt class="docutils literal"&gt;char*&lt;/tt&gt; pointers anymore: they are
now local variables in functions. Instead, functions of API requires two
parameters: the bytes writer and a &lt;tt class="docutils literal"&gt;char*&lt;/tt&gt; parameter. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PyObject * _PyBytesWriter_Finish(_PyBytesWriter *writer, char *str)
&lt;/pre&gt;
&lt;p&gt;The idea is to keep &lt;tt class="docutils literal"&gt;char*&lt;/tt&gt; pointers in functions to keep the most efficient
machine code in loops. The compiler doesn't have to compute complex aliasing
rules to decide if a CPU register can be used or not.&lt;/p&gt;
&lt;p&gt;_PyBytesWriter features:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Optional overallocation: overallocate the buffer by 25% on Windows and 50%
on Linux. Same idea than _PyUnicodeWriter.&lt;/li&gt;
&lt;li&gt;Support &lt;tt class="docutils literal"&gt;bytes&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;bytearray&lt;/tt&gt; type as output format to avoid an expensive
memory copy from &lt;tt class="docutils literal"&gt;bytes&lt;/tt&gt; to &lt;tt class="docutils literal"&gt;bytearray&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Small buffer of 512 bytes allocated on the stack to avoid the need of a
buffer allocated on the heap, before creating the final
&lt;tt class="docutils literal"&gt;bytes&lt;/tt&gt;/&lt;tt class="docutils literal"&gt;bytearray&lt;/tt&gt; object.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A _PyBytesWriter structure must always be allocated on the stack (to get fast
memory allocation of the smaller buffer).&lt;/p&gt;
&lt;p&gt;While _PyUnicodeWriter has a 5 functions and 1 macro to write a single
character, write strings, write a substring, etc. _PyBytesWriter has a single
_PyBytesWriter_WriteBytes() function to write a string, since all other writes
are done directly with regular C code on &lt;tt class="docutils literal"&gt;char*&lt;/tt&gt; pointers.&lt;/p&gt;
&lt;p&gt;The API itself doesn't make the code faster. Disabling overallocation on the
last write and the usage of the small buffer allocated on the stack may be
faster.&lt;/p&gt;
&lt;p&gt;In Python 3.6, I optimized error handlers on various codecs: ASCII, Latin1
and UTF-8. For example, the UTF-8 encoder is now up to 75 times as fast for
error handlers: &lt;tt class="docutils literal"&gt;ignore&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;replace&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;surrogateescape&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;surrogatepass&lt;/tt&gt;. The &lt;tt class="docutils literal"&gt;bytes % int&lt;/tt&gt; instruction became between 30% and 50%
faster on a microbenchmark.&lt;/p&gt;
&lt;p&gt;Later, I replaced &lt;tt class="docutils literal"&gt;char*&lt;/tt&gt; type with &lt;tt class="docutils literal"&gt;void*&lt;/tt&gt; to avoid compiler warnings
in functions using &lt;tt class="docutils literal"&gt;Py_UCS1*&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;unsigned char*&lt;/tt&gt;, unsigned types.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="cpython"></category></entry><entry><title>My contributions to CPython during 2015 Q4</title><link href="https://haypo.github.io/contrib-cpython-2015q4.html" rel="alternate"></link><published>2016-03-01T15:00:00+01:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-03-01:contrib-cpython-2015q4.html</id><summary type="html">&lt;p&gt;My contributions to CPython during 2015 Q4 (october, november, december).&lt;/p&gt;
&lt;p&gt;As usual, I pushed changes of various contributors and helped them to polish
their change.&lt;/p&gt;
&lt;p&gt;I fighted against a recursion error, a regression introduced by my recent work
on the Python test suite.&lt;/p&gt;
&lt;p&gt;I focused on optimizing the bytes type during this quarter. It started with the
issue #24870 opened by INADA Naoki who works on PyMySQL: decoding bytes
using the surrogateescape error handler was the bottleneck of this benchmark.
For me, it was an opportunity for a new attempt to implement a fast &amp;quot;bytes
writer API&amp;quot;.&lt;/p&gt;
&lt;p&gt;I pushed my first change related to &lt;a class="reference external" href="http://faster-cpython.readthedocs.org/fat_python.html"&gt;FAT Python&lt;/a&gt;! Fix parser and AST:
fill lineno and col_offset of &amp;quot;arg&amp;quot; node when compiling AST from Python
objects.&lt;/p&gt;
&lt;div class="section" id="recursion-error"&gt;
&lt;h2&gt;Recursion error&lt;/h2&gt;
&lt;div class="section" id="the-bug-issue-25274"&gt;
&lt;h3&gt;The bug: issue #25274&lt;/h3&gt;
&lt;p&gt;During the previous quarter, I refactored Lib/test/regrtest.py huge file (1,600
lines) into a new Lib/test/libregrtest/ library (8 files). The problem is that
test_sys started to crash with &amp;quot;Fatal Python error: Cannot recover from stack
overflow&amp;quot; on test_recursionlimit_recovery(). The regression was introduced by a
change on regrtest which indirectly added one more Python frame in the code
executing test_sys.&lt;/p&gt;
&lt;p&gt;CPython has a limit on the depth of a call stack: &lt;tt class="docutils literal"&gt;sys.getrecursionlimit()&lt;/tt&gt;,
1000 by default. The limit is a weak protection against overflow of the C
stack. Weak because it only counts Python frames, intermediate C functions may
allocate a lot of memory on the stack.&lt;/p&gt;
&lt;p&gt;When we reach the limit, an &amp;quot;overflow&amp;quot; flag is set, but we still allow up to
limit+50 frames, because handling a RecursionError may need a few more frames.
The overflow flag is cleared when the stack level goes below a &amp;quot;low-water
mark&amp;quot;.&lt;/p&gt;
&lt;p&gt;After the regrtest change, test_recursionlimit_recovery() was called at stack
level 36. Before, it was called at level 35. The test triggers a RecursionError.
The problem is that we never goes again below the low-water mark, so the
overflow flag is never cleared.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-fix"&gt;
&lt;h3&gt;The fix&lt;/h3&gt;
&lt;p&gt;Another problem is that the function used to compute the &amp;quot;low-level mark&amp;quot; was
not monotonic:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
if limit &amp;gt; 100:
    low_water_mark = limit - 50
else:
    low_water_mark = 3 * limit // 4
&lt;/pre&gt;
&lt;p&gt;The gap occurs near a limit of 100 frames:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;limit = 99 =&amp;gt; low_level_mark = 74&lt;/li&gt;
&lt;li&gt;limit = 100 =&amp;gt; low_level_mark = 75&lt;/li&gt;
&lt;li&gt;limit = 101 =&amp;gt; low_level_mark = 51&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The formula was replaced with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
if limit &amp;gt; 200:
    low_water_mark = limit - 50
else:
    low_water_mark = 3 * limit // 4
&lt;/pre&gt;
&lt;p&gt;The fix (&lt;a class="reference external" href="https://hg.python.org/cpython/rev/eb0c76442cee"&gt;change eb0c76442cee&lt;/a&gt;) modified the
&lt;tt class="docutils literal"&gt;sys.setrecursionlimit()&lt;/tt&gt; function to raise a &lt;tt class="docutils literal"&gt;RecursionError&lt;/tt&gt; exception if
the new limit is too low depending on the &lt;em&gt;current&lt;/em&gt; stack depth.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="optimizations"&gt;
&lt;h2&gt;Optimizations&lt;/h2&gt;
&lt;p&gt;As usual for performance, Serhiy Storchaka was very helpful on reviews, to run
independant benchmarks, etc.&lt;/p&gt;
&lt;p&gt;Optimizations on the &lt;tt class="docutils literal"&gt;bytes&lt;/tt&gt; type, ASCII, Latin1 and UTF-8 codecs:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #25318: Add _PyBytesWriter API. Add a new private API to optimize
Unicode encoders. It uses a small buffer of 512 bytes allocated on the stack
and supports configurable overallocation.&lt;/li&gt;
&lt;li&gt;Use _PyBytesWriter API for UCS1 (ASCII and Latin1) and UTF-8 encoders. Enable
overallocation for the UTF-8 encoder with error handlers.&lt;/li&gt;
&lt;li&gt;unicode_encode_ucs1(): initialize collend to collstart+1 to not check the
current character twice, we already know that it is not ASCII.&lt;/li&gt;
&lt;li&gt;Issue #25267: The UTF-8 encoder is now up to 75 times as fast for error
handlers: &lt;tt class="docutils literal"&gt;ignore&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;replace&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;surrogateescape&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;surrogatepass&lt;/tt&gt;.
Patch co-written with Serhiy Storchaka.&lt;/li&gt;
&lt;li&gt;Issue #25301: The UTF-8 decoder is now up to 15 times as fast for error
handlers: &lt;tt class="docutils literal"&gt;ignore&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;replace&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;surrogateescape&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Issue #25318: Optimize backslashreplace and xmlcharrefreplace error handlers
in UTF-8 encoder. Optimize also backslashreplace error handler for ASCII and
Latin1 encoders.&lt;/li&gt;
&lt;li&gt;Issue #25349: Optimize bytes % args using the new private _PyBytesWriter API&lt;/li&gt;
&lt;li&gt;Optimize error handlers of ASCII and Latin1 encoders when the replacement
string is pure ASCII: use _PyBytesWriter_WriteBytes(), don't check individual
character.&lt;/li&gt;
&lt;li&gt;Issue #25349: Optimize bytes % int. Formatting is between 30% and 50% faster
on a microbenchmark.&lt;/li&gt;
&lt;li&gt;Issue #25357: Add an optional newline paramer to binascii.b2a_base64().
base64.b64encode() uses it to avoid a memory copy.&lt;/li&gt;
&lt;li&gt;Issue #25353: Optimize unicode escape and raw unicode escape encoders: use
the new _PyBytesWriter API.&lt;/li&gt;
&lt;li&gt;Rewrite PyBytes_FromFormatV() using _PyBytesWriter API&lt;/li&gt;
&lt;li&gt;Issue #25399: Optimize bytearray % args. Most formatting operations are now
between 2.5 and 5 times faster.&lt;/li&gt;
&lt;li&gt;Issue #25401: Optimize bytes.fromhex() and bytearray.fromhex(): they are now
between 2x and 3.5x faster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="changes"&gt;
&lt;h2&gt;Changes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #25003: On Solaris 11.3 or newer, os.urandom() now uses the getrandom()
function instead of the getentropy() function. The getentropy() function is
blocking to generate very good quality entropy, os.urandom() doesn't need
such high-quality entropy.&lt;/li&gt;
&lt;li&gt;Issue #22806: Add &lt;tt class="docutils literal"&gt;python &lt;span class="pre"&gt;-m&lt;/span&gt; test &lt;span class="pre"&gt;--list-tests&lt;/span&gt;&lt;/tt&gt; command to list tests.&lt;/li&gt;
&lt;li&gt;Issue #25670: Remove duplicate getattr() in ast.NodeTransformer&lt;/li&gt;
&lt;li&gt;Issue #25557: Refactor _PyDict_LoadGlobal(). Don't fallback to
PyDict_GetItemWithError() if the hash is unknown: compute the hash instead.
Add also comments to explain the _PyDict_LoadGlobal() optimization.&lt;/li&gt;
&lt;li&gt;Issue #25868: Try to make test_eintr.test_sigwaitinfo() more reliable
especially on slow buildbots&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="changes-specific-to-python-2-7"&gt;
&lt;h2&gt;Changes specific to Python 2.7&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Closes #25742: locale.setlocale() now accepts a Unicode string for its second
parameter.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="bugfixes"&gt;
&lt;h2&gt;Bugfixes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Fix regrtest --coverage on Windows&lt;/li&gt;
&lt;li&gt;Fix pytime on OpenBSD&lt;/li&gt;
&lt;li&gt;More fixes for test_eintr on FreeBSD&lt;/li&gt;
&lt;li&gt;Close #25373: Fix regrtest --slow with interrupted test&lt;/li&gt;
&lt;li&gt;Issue #25555: Fix parser and AST: fill lineno and col_offset of &amp;quot;arg&amp;quot; node
when compiling AST from Python objects. First contribution related
to FAT Python ;-)&lt;/li&gt;
&lt;li&gt;Issue #25696: Fix installation of Python on UNIX with make -j9.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</summary><category term="cpython"></category></entry><entry><title>My contributions to CPython during 2015 Q3</title><link href="https://haypo.github.io/contrib-cpython-2015q3.html" rel="alternate"></link><published>2016-02-18T01:00:00+01:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-02-18:contrib-cpython-2015q3.html</id><summary type="html">&lt;p&gt;A few years ago, someone asked me: &amp;quot;Why do you contribute to CPython? Python is
perfect, there are no more bugs, right?&amp;quot;. The article list most of my
contributions to CPython during 2015 Q3 (july, august, september). It gives an
idea of which areas of Python are not perfect yet :-)&lt;/p&gt;
&lt;p&gt;The major event in Python of this quarter was the release of Python 3.5.0.&lt;/p&gt;
&lt;p&gt;As usual, I helped various contributors to refine their changes and I pushed
their final changes.&lt;/p&gt;
&lt;div class="section" id="freebsd-kernel-bug"&gt;
&lt;h2&gt;FreeBSD kernel bug&lt;/h2&gt;
&lt;p&gt;It took me a while to polish the implementation of the &lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0475/"&gt;PEP 475 (retry syscall
on EINTR)&lt;/a&gt; especially its unit
test &lt;tt class="docutils literal"&gt;test_eintr&lt;/tt&gt;. The unit test is supposed to test Python, but as usual,
it also tests indirectly the operating system.&lt;/p&gt;
&lt;p&gt;I spent some days investigating a random hang on the FreeBSD buildbots: &lt;a class="reference external" href="https://bugs.python.org/issue25122"&gt;issue
#25122&lt;/a&gt;. I quickly found the guilty test
(test_eintr.test_open), but it took me a while to understand that it was a
kernel bug in the FIFO driver. Hopefully at the end, I was able to reproduce
the bug with a short C program in my FreeBSD VM. It is the best way to ask a
fix upstream.&lt;/p&gt;
&lt;p&gt;My &lt;a class="reference external" href="https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=203162"&gt;FreeBSD bug report #203162&lt;/a&gt; (&amp;quot;when close(fd)
on a fifo fails with EINTR, the file descriptor is not really closed&amp;quot;) was
quickly fixed. The FreeBSD team is reactive!&lt;/p&gt;
&lt;p&gt;I like free softwares because it's possible to investigate bugs deep in the
code, and it's usually quick to get a fix.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="timestamp-rounding-issue"&gt;
&lt;h2&gt;Timestamp rounding issue&lt;/h2&gt;
&lt;p&gt;Even if the &lt;a class="reference external" href="http://bugs.python.org/issue23517"&gt;issue #23517&lt;/a&gt; is well defined
and simple to fix, it took me days (weeks?) to understand exactly how
timestamps are supposed to be rounded and agree on the &amp;quot;right&amp;quot; rounding method.
Alexander Belopolsky reminded me the important property:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
(datetime(1970,1,1) + timedelta(seconds=t)) == datetime.utcfromtimestamp(t)
&lt;/pre&gt;
&lt;p&gt;Tim Peters helped me to understand why Python rounds to nearest with ties going
away from zero (ROUND_HALF_UP) in &lt;tt class="docutils literal"&gt;round(float)&lt;/tt&gt; and other functions. At
the first look, the rounding method doesn't look natural nor logical:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
&amp;gt;&amp;gt;&amp;gt; round(0.5)
0
&amp;gt;&amp;gt;&amp;gt; round(1.5)
2
&lt;/pre&gt;
&lt;p&gt;See my previous article on the _PyTime API for the long story of rounding
methods between Python 3.2 and Python 3.6.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="enhancements"&gt;
&lt;h2&gt;Enhancements&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;type_call() now detect C bugs in type __new__() and __init__() methods.&lt;/li&gt;
&lt;li&gt;Issue #25220: Enhancements of the test runner: add more info when regrtest runs
tests in parallel, fix some features of regrtest, add functional tests to
test_regrtest.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="optimizations"&gt;
&lt;h2&gt;Optimizations&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #25227: Optimize ASCII and latin1 encoders with the &lt;tt class="docutils literal"&gt;surrogateescape&lt;/tt&gt;
error handler: the encoders are now up to 3 times as fast.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="changes"&gt;
&lt;h2&gt;Changes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Polish the implementation of the PEP 475 (retry syscall on EINTR)&lt;/li&gt;
&lt;li&gt;Work on the &amp;quot;What's New in Python 3.5&amp;quot; document: add my changes
(PEP 475, socket timeout, os.urandom)&lt;/li&gt;
&lt;li&gt;Work on asyncio: fix ResourceWarning warnings, fixes specific to Windows&lt;/li&gt;
&lt;li&gt;test_time: rewrite rounding tests of the private pytime API&lt;/li&gt;
&lt;li&gt;Issue #24707: Remove an assertion in monotonic clock. Don't check anymore at
runtime that the monotonic clock doesn't go backward.  Yes, it happens! It
occurs sometimes each month on a Debian buildbot slave running in a VM.&lt;/li&gt;
&lt;li&gt;test_eintr: replace os.fork() with subprocess (fork+exec) to make the test
more reliable&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="changes-specific-to-python-2-7"&gt;
&lt;h2&gt;Changes specific to Python 2.7&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Backport python-gdb.py changes: enhance py-bt command&lt;/li&gt;
&lt;li&gt;Issue #23375: Fix test_py3kwarn for modules implemented in C&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="bug-fixes"&gt;
&lt;h2&gt;Bug fixes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Closes #23247: Fix a crash in the StreamWriter.reset() of CJK codecs&lt;/li&gt;
&lt;li&gt;Issue #24732, #23834: Fix sock_accept_impl() on Windows. Regression of the
PEP 475 (retry syscall on EINTR)&lt;/li&gt;
&lt;li&gt;test_gdb: fix regex to parse the GDB version and fix ResourceWarning on error&lt;/li&gt;
&lt;li&gt;Fix test_warnings: don't modify warnings.filters to fix random failures of
the test.&lt;/li&gt;
&lt;li&gt;Issue #24891: Fix a race condition at Python startup if the file descriptor
of stdin (0), stdout (1) or stderr (2) is closed while Python is creating
sys.stdin, sys.stdout and sys.stderr objects.&lt;/li&gt;
&lt;li&gt;Issue #24684: socket.socket.getaddrinfo() now calls
PyUnicode_AsEncodedString() instead of calling the encode() method of the
host, to handle correctly custom string with an encode() method which doesn't
return a byte string. The encoder of the IDNA codec is now called directly
instead of calling the encode() method of the string.&lt;/li&gt;
&lt;li&gt;Issue #25118: Fix a regression of Python 3.5.0 in os.waitpid() on Windows.
Add an unit test on os.waitpid()&lt;/li&gt;
&lt;li&gt;Issue #25122: Fix test_eintr, kill child process on error&lt;/li&gt;
&lt;li&gt;Issue #25155: Add _PyTime_AsTimevalTime_t() function to fix a regression:
support again years after 2038.&lt;/li&gt;
&lt;li&gt;Issue #25150: Hide the private _Py_atomic_xxx symbols from the public
Python.h header to fix a compilation error with OpenMP. PyThreadState_GET()
becomes an alias to PyThreadState_Get() to avoid ABI incompatibilies.&lt;/li&gt;
&lt;li&gt;Issue #25003: On Solaris 11.3 or newer, os.urandom() now uses the getrandom()
function instead of the getentropy() function.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</summary><category term="cpython"></category></entry><entry><title>History of the Python private C API _PyTime</title><link href="https://haypo.github.io/pytime.html" rel="alternate"></link><published>2016-02-17T22:00:00+01:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-02-17:pytime.html</id><summary type="html">&lt;p&gt;I added functions to the private &amp;quot;pytime&amp;quot; library to convert timestamps from/to
various formats. I expected to spend a few days, at the end I spent 3 years
(2012-2015) on them!&lt;/p&gt;
&lt;div class="section" id="python-3-3"&gt;
&lt;h2&gt;Python 3.3&lt;/h2&gt;
&lt;p&gt;In 2012, I proposed the &lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0410/"&gt;PEP 410 -- Use decimal.Decimal type for timestamps&lt;/a&gt; because storing timestamps as
floating point numbers looses precision. The PEP was rejected because it
modified many functions and had a bad API. At least, os.stat() got 3 new fields
(atime_ns, mtime_ns, ctime_ns): timestamps  as a number of nanoseconds
(&lt;tt class="docutils literal"&gt;int&lt;/tt&gt;).&lt;/p&gt;
&lt;p&gt;My &lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0418/"&gt;PEP 418 -- Add monotonic time, performance counter, and process time
functions&lt;/a&gt; was accepted, Python
3.3 got a new &lt;tt class="docutils literal"&gt;time.monotonic()&lt;/tt&gt; function (and a few others). Again, I spent
much more time than I expected on a problem which looked simple at the first
look.&lt;/p&gt;
&lt;p&gt;With the &lt;a class="reference external" href="http://bugs.python.org/issue14180"&gt;issue #14180&lt;/a&gt;, I added functions
to convert timestamps to the private &amp;quot;pytime&amp;quot; API to factorize the code of
various modules. Timestamps were rounded towards +infinity (ROUND_CEILING), but
it was not a deliberate choice.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-3-4"&gt;
&lt;h2&gt;Python 3.4&lt;/h2&gt;
&lt;p&gt;To fix correctly a performance issue in asyncio (&lt;a class="reference external" href="https://bugs.python.org/issue20311"&gt;issue20311&lt;/a&gt;), I added two rounding modes to the
pytime API: _PyTime_ROUND_DOWN (round towards zero), and _PyTime_ROUND_UP
(round away from zero). Polling for events (ex: using &lt;tt class="docutils literal"&gt;select.select()&lt;/tt&gt;) with
a non-zero timestamp must not call the underlying C level in non-blocking mode.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-3-5"&gt;
&lt;h2&gt;Python 3.5&lt;/h2&gt;
&lt;p&gt;When working on the &lt;a class="reference external" href="https://bugs.python.org/issue22117"&gt;issue #22117&lt;/a&gt;, I
noticed that the implementation of rounding methods was buggy for negative
timestamps. I replaced the _PyTime_ROUND_DOWN with _PyTime_ROUND_FLOOR (round
towards minus infinity), and _PyTime_ROUND_UP with _PyTime_ROUND_CEILING (round
towards infinity).&lt;/p&gt;
&lt;p&gt;This issue also introduced a new private &lt;tt class="docutils literal"&gt;_PyTime_t&lt;/tt&gt; type to support
nanosecond resolution.  The type is an opaque integer type to store timestamps.
In practice, it's a signed 64-bit integer. Since it's an integer, it's easy and
natural to compute the sum or differecence of two timestamps: &lt;tt class="docutils literal"&gt;t1 + t2&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;t2 - t1&lt;/tt&gt;. I added _PyTime_XXX() functions to create a timestamp and
_PyTime_AsXXX() functions to convert a timestamp to a different format.&lt;/p&gt;
&lt;p&gt;I had to keep three _PyTime_ObjectToXXX() functions for fromtimestamp() methods
of the datetime module. These methods must support extreme timestamps (year
1..9999), whereas _PyTime_t is &amp;quot;limited&amp;quot; to a delta of +/- 292 years (year
1678..2262).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-3-6"&gt;
&lt;h2&gt;Python 3.6&lt;/h2&gt;
&lt;p&gt;In 2015, the &lt;a class="reference external" href="http://bugs.python.org/issue23517"&gt;issue #23517&lt;/a&gt; reported that
Python 2 and Python 3 don't use the same rounding method in
datetime.datetime.fromtimestamp(): there was a difference of 1 microsecond.&lt;/p&gt;
&lt;p&gt;After a long discussion, I modified fromtimestamp() methods of the datetime
module to round to nearest with ties going away from zero (ROUND_HALF_UP), as
done in Python 2.7, as round() in all Python versions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;It took me three years to stabilize the API and fix all issues. Well, I didn't
spend all my days on it, but it shows that handling time is not a simple issue.&lt;/p&gt;
&lt;p&gt;At the Python level, nothing changed, timestamps are still stored as float
(except of the 3 new fieleds of os.stat()).&lt;/p&gt;
&lt;p&gt;Python 3.5 only supports timezones with fixed offset, it does not support the
locale timestamp for example. Timezones are still an hot topic: the
&lt;a class="reference external" href="https://mail.python.org/mailman/listinfo/datetime-sig"&gt;datetime-sig mailing list&lt;/a&gt; was created to
enhance timezone support in Python.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="cpython"></category></entry><entry><title>Status of the FAT Python project, January 12, 2016</title><link href="https://haypo.github.io/fat-python-status-janv12-2016.html" rel="alternate"></link><published>2016-01-12T13:42:00+01:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-01-12:fat-python-status-janv12-2016.html</id><summary type="html">&lt;a class="reference external image-reference" href="http://faster-cpython.readthedocs.org/fat_python.html"&gt;&lt;img alt="FAT Python project" class="align-right" src="https://haypo.github.io/images/fat_python.jpg" /&gt;&lt;/a&gt;
&lt;p&gt;Previous status: &lt;a class="reference external" href="https://haypo.github.io/fat-python-status-nov26-2015.html"&gt;Status of the FAT Python project, November 26, 2015&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="summary"&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;New optimizations implemented:&lt;ul&gt;
&lt;li&gt;constant propagation&lt;/li&gt;
&lt;li&gt;constant folding&lt;/li&gt;
&lt;li&gt;dead code elimination&lt;/li&gt;
&lt;li&gt;simplify iterable&lt;/li&gt;
&lt;li&gt;replace builtin __debug__ variable with its value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Major API refactoring to make the API more generic and reusable by other
projects, and maybe different use case.&lt;/li&gt;
&lt;li&gt;Work on 3 different Python Enhancement Proposals (PEP): API for pluggable
static optimizers and function specialization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The two previously known major bugs, &amp;quot;Wrong Line Numbers (and Tracebacks)&amp;quot; and
&amp;quot;exec(code, dict)&amp;quot;, are now fixed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-enhancement-proposals-pep"&gt;
&lt;h2&gt;Python Enhancement Proposals (PEP)&lt;/h2&gt;
&lt;p&gt;I proposed an API for to support function specialization and static optimizers.
I splitted changes in 3 different Python Enhancement Proposals (PEP):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0509/"&gt;PEP 509 - Add a private version to dict&lt;/a&gt;: &amp;quot;Add a new private version to
builtin &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt; type, incremented at each change, to implement fast guards
on namespaces.&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0510/"&gt;PEP 510 - Specialize functions&lt;/a&gt;: &amp;quot;Add functions to the Python C
API to specialize pure Python functions: add specialized codes with guards.
It allows to implement static optimizers respecting the Python semantics.&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0511/"&gt;PEP 511 - API for AST transformers&lt;/a&gt;: &amp;quot;Propose an API to
support AST transformers.&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The PEP 509 was sent to the python-ideas mailing list for a first round, and
then to python-dev mailing list.  The PEP 510 was sent to python-ideas to a
first round. The last PEP was not published yet, I'm still working on it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="major-api-refactor"&gt;
&lt;h2&gt;Major API refactor&lt;/h2&gt;
&lt;p&gt;The API has been deeply refactored to write the Python Enhancement Proposals.&lt;/p&gt;
&lt;p&gt;First set of changes for function specialization (PEP 510):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;astoptimizer now adds &lt;tt class="docutils literal"&gt;import fat&lt;/tt&gt; to optimized code when specialization is
used&lt;/li&gt;
&lt;li&gt;Remove the function subtype: add directly the &lt;tt class="docutils literal"&gt;specialize()&lt;/tt&gt; method to
functions&lt;/li&gt;
&lt;li&gt;Add support of any callable object to &lt;tt class="docutils literal"&gt;func.specialize()&lt;/tt&gt;, not only code
object (bytecode)&lt;/li&gt;
&lt;li&gt;Create guard objects:&lt;ul&gt;
&lt;li&gt;fat.Guard&lt;/li&gt;
&lt;li&gt;fat.GuardArgType&lt;/li&gt;
&lt;li&gt;fat.GuardBuiltins&lt;/li&gt;
&lt;li&gt;fat.GuardDict&lt;/li&gt;
&lt;li&gt;fat.GuardFunc&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add functions to create guards:&lt;ul&gt;
&lt;li&gt;fat.GuardGlobals&lt;/li&gt;
&lt;li&gt;fat.GuardTypeDict&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Move code.replace_consts() to fat.replace_consts()&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Second set of changes for AST transformers (PEP 511):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Add sys.implementation.ast_transformers and sys.implementation.optim_tag&lt;/li&gt;
&lt;li&gt;Rename sys.asthook to sys.ast_transformers&lt;/li&gt;
&lt;li&gt;Add -X fat command line option to enable the FAT mode: register the
astoptimizer in AST transformers&lt;/li&gt;
&lt;li&gt;Replace -F command line option with -o OPTIM_TAG&lt;/li&gt;
&lt;li&gt;Remove sys.flags.fat (Python flag) and Py_FatPython (C variable)&lt;/li&gt;
&lt;li&gt;Rewrite how an AST transformer is registered&lt;/li&gt;
&lt;li&gt;importlib skips .py if optim_tag is not 'opt' and required AST transformers
are missing. Raise ImportError if the .pyc file is missing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Third set of changes for dictionary versionning, updates after the first round
of the PEP 509 on python-ideas:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Remove dict.__version__ read-only property: the version is now only
accessible from the C API&lt;/li&gt;
&lt;li&gt;Change the type of the C field &lt;tt class="docutils literal"&gt;ma_version&lt;/tt&gt; from &lt;tt class="docutils literal"&gt;size_t&lt;/tt&gt; to &lt;tt class="docutils literal"&gt;unsigned
PY_INT64_T&lt;/tt&gt; to also use 64-bit unsigned integer on 32-bit platforms. The
risk of missing a change in a guard with a 32-bit version is too high,
whereas the risk with a 64-bit version is very very low.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fourth set of changes for function specialization, updates after the first round
of the PEP 510 on python-ideas:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Remove func.specialize() and func.get_specialized() at the Python level,
replace them with C functions. Expose them again as fat.specialize(func, ...)
and fat.get_specialized(func)&lt;/li&gt;
&lt;li&gt;fat.get_specialized() now returns a list of tuples, instead of a list of dict&lt;/li&gt;
&lt;li&gt;Make fat.Guard type private: rename it to fat._Guard&lt;/li&gt;
&lt;li&gt;Add fat.PyGuard: toy to implement a guard in pure Python&lt;/li&gt;
&lt;li&gt;Guard C API: rename first_check to init and support reporting errors&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="change-log"&gt;
&lt;h2&gt;Change log&lt;/h2&gt;
&lt;p&gt;Detailed changes of the FAT Python between November 24, 2015 and January 12,
2016.&lt;/p&gt;
&lt;div class="section" id="end-of-november"&gt;
&lt;h3&gt;End of november&lt;/h3&gt;
&lt;p&gt;Major change:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Add a __version__ read-only property to dict, remove the verdict subtype of
dict. As a consequence, dictionary guards now hold a strong reference to the
dict value&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Minor changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Allocate dynamically memory for specialized code and guards, don't use fixed-size
arrays anymore&lt;/li&gt;
&lt;li&gt;astoptimizer: enhance scope detection&lt;/li&gt;
&lt;li&gt;optimize astoptimizer: don't copy a whole AST tree anymore with
copy.deepcopy(), only copy modified nodes.&lt;/li&gt;
&lt;li&gt;Add Config.max_constant_size&lt;/li&gt;
&lt;li&gt;Reenable checks on cell variables: allow cell variables if they are the same&lt;/li&gt;
&lt;li&gt;Reenable optimizations on methods calling super(), but never copy super()
builtin to constants. If super() is replaced with a string, the required free
variable (reference to the current class) is not created by the compiler&lt;/li&gt;
&lt;li&gt;Add PureBuiltin config&lt;/li&gt;
&lt;li&gt;NodeVisitor now calls generic_visit() before visit_XXX()&lt;/li&gt;
&lt;li&gt;Loop unrolling now also optimizes tuple iterators&lt;/li&gt;
&lt;li&gt;At the end of Python initialization, create a copy of the builtins dictionary
to be able later to detect if a builtin name was replaced.&lt;/li&gt;
&lt;li&gt;Implement collections.UserDict.__version__&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="december-first-half"&gt;
&lt;h3&gt;December (first half)&lt;/h3&gt;
&lt;p&gt;Major changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Implement 4 new optimizations:&lt;ul&gt;
&lt;li&gt;constant propagation&lt;/li&gt;
&lt;li&gt;constant folding&lt;/li&gt;
&lt;li&gt;replace builtin __debug__ variable with its value&lt;/li&gt;
&lt;li&gt;dead code elimination&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add support of per module configuration using an __astoptimizer__ variable&lt;/li&gt;
&lt;li&gt;code.co_lnotab now supports negative line number delta.  Change the type of
line number delta in co_lnotab from unsigned 8-bit integer to signed 8-bit
integer. This change fixes almost all issues about line numbers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Minor changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Change .pyc magic number to 3600&lt;/li&gt;
&lt;li&gt;Remove unused fat.specialized_method() function&lt;/li&gt;
&lt;li&gt;Remove Lib/fat.py, rename Modules/_fat.c to Modules/fat.c: fat module is now
only implemented in C&lt;/li&gt;
&lt;li&gt;Fix more tests of the Python test suite&lt;/li&gt;
&lt;li&gt;A builtin guard now adds a guard on globals. Ignore also the specialization
if globals()[name] already exists.&lt;/li&gt;
&lt;li&gt;Ignore duplicated guards&lt;/li&gt;
&lt;li&gt;Implement namespace following the control flow for constant propagation&lt;/li&gt;
&lt;li&gt;Config.max_int_bits becomes a simple integer&lt;/li&gt;
&lt;li&gt;Fix bytecode compilation for tuple constants. Don't merge (0, 0) and (0.0,
0.0) constants, they are different.&lt;/li&gt;
&lt;li&gt;Call more builtin functions&lt;/li&gt;
&lt;li&gt;Optimize the optimizer: write a metaclass to discover visitors when the class
is created, not when the class is instanciated&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="december-second-half"&gt;
&lt;h3&gt;December (second half)&lt;/h3&gt;
&lt;p&gt;Major changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Implement &amp;quot;simplify iterable&amp;quot; optimization. The loop unrolling optimization
now relies on it to replace &lt;tt class="docutils literal"&gt;range(n)&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Split the function optimization in two stages: first apply optimizations
which don't require specialization, then apply optimizations which
require specialization.&lt;/li&gt;
&lt;li&gt;Replace the builtin __fat__ variable with a new sys.flags.fat flag&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Minor changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Extend optimizations to optimize more cases (more builtins, more loop
unrolling, remove more dead code, etc.)&lt;/li&gt;
&lt;li&gt;Add Config.logger attribute. astoptimize logs into sys.stderr when Python is
started in verbose mode (python3 -v)&lt;/li&gt;
&lt;li&gt;Move func.patch_constants() to code.replace_consts()&lt;/li&gt;
&lt;li&gt;Enhance marshal to fix tests: call frozenset() to get the empty frozenset
singleton&lt;/li&gt;
&lt;li&gt;Don't remove code which must raise a SyntaxError. Don't remove code
containing the continue instruction.&lt;/li&gt;
&lt;li&gt;Restrict GlobalNonlocalVisitor to the current namespace&lt;/li&gt;
&lt;li&gt;Emit logs when optimizations are skipped&lt;/li&gt;
&lt;li&gt;Use some maths to avoid optimization pow() if result is an integer and will
be larger than the configuration. For example, don't optimize 2 ** (2**100).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="january"&gt;
&lt;h3&gt;January&lt;/h3&gt;
&lt;p&gt;Major changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;astoptimizer now produces a single builtin guard with all names,
instead of a guard per name.&lt;/li&gt;
&lt;li&gt;Major API refactoring detailed in a dedicated section above&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Minor changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Start to write PEPs&lt;/li&gt;
&lt;li&gt;Dictionary guards now expect a list of names, instead of a single name, to
reduce the cost of guards.&lt;/li&gt;
&lt;li&gt;GuardFunc now uses a strong reference to the function, instead of a weak
reference to simplify the code&lt;/li&gt;
&lt;li&gt;Initialize dictionary version to 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</summary><category term="optimization"></category><category term="fatpython"></category></entry><entry><title>Status of the FAT Python project, November 26, 2015</title><link href="https://haypo.github.io/fat-python-status-nov26-2015.html" rel="alternate"></link><published>2015-11-26T17:30:00+01:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2015-11-26:fat-python-status-nov26-2015.html</id><summary type="html">&lt;a class="reference external image-reference" href="http://faster-cpython.readthedocs.org/fat_python.html"&gt;&lt;img alt="FAT Python project" class="align-right" src="https://haypo.github.io/images/fat_python.jpg" /&gt;&lt;/a&gt;
&lt;p&gt;Previous status: [python-dev] &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2015-November/142113.html"&gt;Second milestone of FAT Python&lt;/a&gt;
(Nov 4, 2015).&lt;/p&gt;
&lt;div class="section" id="documentation"&gt;
&lt;h2&gt;Documentation&lt;/h2&gt;
&lt;p&gt;I combined the documentation of various optimizations projects into a single
documentation: &lt;a class="reference external" href="http://faster-cpython.readthedocs.org/"&gt;Faster CPython&lt;/a&gt;.
My previous optimizations projects:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://faster-cpython.readthedocs.org/old_ast_optimizer.html"&gt;&amp;quot;old&amp;quot; astoptimizer&lt;/a&gt; (now
replaced with a &amp;quot;new&amp;quot; astoptimizer included in the FAT Python)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://faster-cpython.readthedocs.org/registervm.html"&gt;registervm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://faster-cpython.readthedocs.org/readonly.html"&gt;read-only Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The FAT Python project has its own page: &lt;a class="reference external" href="http://faster-cpython.readthedocs.org/fat_python.html"&gt;FAT Python project&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="copy-builtins-to-constants-optimization"&gt;
&lt;h2&gt;Copy builtins to constants optimization&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;LOAD_GLOBAL&lt;/tt&gt; instruction is used to load a builtin function.  The
instruction requires two dictionary lookup: one in the global namespace (which
almost always fail) and then in the builtin namespaces.&lt;/p&gt;
&lt;p&gt;It's rare to replace builtins, so the idea here is to replace the dynamic
&lt;tt class="docutils literal"&gt;LOAD_GLOBAL&lt;/tt&gt; instruction with a static &lt;tt class="docutils literal"&gt;LOAD_CONST&lt;/tt&gt; instruction which
loads the function from a C array, a fast O(1) lookup.&lt;/p&gt;
&lt;p&gt;It is not possible to inject a builtin function during the compilation. Python
code objects are serialized by the marshal module which only support simple
types like integers, strings and tuples, not functions. The trick is to modify
the constants at runtime when the module is loaded. I added a new
&lt;tt class="docutils literal"&gt;patch_constants()&lt;/tt&gt; method to functions.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def log(message):
    print(message)
&lt;/pre&gt;
&lt;p&gt;This function is specialized to:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def log(message):
    'LOAD_GLOBAL print'(message)
log.patch_constants({'LOAD_GLOBAL print': print})
&lt;/pre&gt;
&lt;p&gt;The specialized bytecode uses two guards on builtin and global namespaces to
disable the optimization if the builtin function is replaced.&lt;/p&gt;
&lt;p&gt;See &lt;a class="reference external" href="https://faster-cpython.readthedocs.org/fat_python.html#copy-builtin-functions-to-constants"&gt;Copy builtin functions to constants&lt;/a&gt;
for more information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="loop-unrolling-optimization"&gt;
&lt;h2&gt;Loop unrolling optimization&lt;/h2&gt;
&lt;p&gt;A simple optimization is to &amp;quot;unroll&amp;quot; a loop to reduce the cost of loops. The
optimization generates assignement statements (for the loop index variable)
and duplicates the loop body.&lt;/p&gt;
&lt;p&gt;Example with a &lt;tt class="docutils literal"&gt;range()&lt;/tt&gt; iterator:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def func():
    for i in (1, 2, 3):
        print(i)
&lt;/pre&gt;
&lt;p&gt;The function is specialized to:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def func():
    i = 1
    print(i)

    i = 2
    print(i)

    i = 3
    print(i)
&lt;/pre&gt;
&lt;p&gt;If the iterator uses the builtin &lt;tt class="docutils literal"&gt;range&lt;/tt&gt; function, two guards are
required on builtin and global namespaces.&lt;/p&gt;
&lt;p&gt;The optimization also handles tuple iterator. No guard is needed in this case
(the code is always optimized).&lt;/p&gt;
&lt;p&gt;See &lt;a class="reference external" href="https://faster-cpython.readthedocs.org/fat_python.html#loop-unrolling"&gt;Loop unrolling&lt;/a&gt;
for more information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="lot-of-enhancements-of-the-ast-optimizer"&gt;
&lt;h2&gt;Lot of enhancements of the AST optimizer&lt;/h2&gt;
&lt;p&gt;New optimizations helped to find bugs in the &lt;a class="reference external" href="https://faster-cpython.readthedocs.org/new_ast_optimizer.html"&gt;AST optimizer&lt;/a&gt;. Many fixes
and various enhancements were done in the AST optimizer.&lt;/p&gt;
&lt;p&gt;The number of lines of code more than doubled: 500 to 1200 lines.&lt;/p&gt;
&lt;p&gt;Optimization: &lt;tt class="docutils literal"&gt;copy.deepcopy()&lt;/tt&gt; is no more used to duplicate a full tree. The
new &lt;tt class="docutils literal"&gt;NodeTransformer&lt;/tt&gt; class now only copies a single node, if at least one
field is modified.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;VariableVisitor&lt;/tt&gt; class which detects local and global variables was
heavily modified. It understands much more kinds of AST node: &lt;tt class="docutils literal"&gt;For&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;AugAssign&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;AsyncFunctionDef&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;ClassDef&lt;/tt&gt;, etc. It now also detects non-local
variables (&lt;tt class="docutils literal"&gt;nonlocal&lt;/tt&gt; keyword). The scope is now limited to the current
function, it doesn't enter inside nested &lt;tt class="docutils literal"&gt;DictComp&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;FunctionDef&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;Lambda&lt;/tt&gt;, etc. These nodes create a new separated namespace.&lt;/p&gt;
&lt;p&gt;The optimizer is now able to optimize a function without guards: it's needed to
unroll a loop using a tuple as iterator.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="known-bugs"&gt;
&lt;h2&gt;Known bugs&lt;/h2&gt;
&lt;p&gt;See the &lt;a class="reference external" href="https://hg.python.org/sandbox/fatpython/file/0d30dba5fa64/TODO.rst"&gt;TODO.rst file&lt;/a&gt; for
known bugs.&lt;/p&gt;
&lt;div class="section" id="wrong-line-numbers-and-tracebacks"&gt;
&lt;h3&gt;Wrong Line Numbers (and Tracebacks)&lt;/h3&gt;
&lt;p&gt;AST nodes have &lt;tt class="docutils literal"&gt;lineno&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;col_offset&lt;/tt&gt; fields, so an AST optimizer is not
&amp;quot;supposed&amp;quot; to break line numbers. In practice, line numbers, and so tracebacks,
are completly wrong in FAT mode. The problem is probably that AST optimizer can
copy and move instructions. Line numbers are no more motononic. CPython
probably don't handle this case (negative line delta).&lt;/p&gt;
&lt;p&gt;It should be possible to fix it, but right now I prefer to focus on new
optimizations and fix other bugs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="exec-code-dict"&gt;
&lt;h3&gt;exec(code, dict)&lt;/h3&gt;
&lt;p&gt;In FAT mode, some optimizations require guards on the global namespace.
If &lt;tt class="docutils literal"&gt;exec()&lt;/tt&gt; if called with a Python &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt; for globals, an exception
is raised because &lt;tt class="docutils literal"&gt;func.specialize()&lt;/tt&gt; requires a &lt;tt class="docutils literal"&gt;fat.verdict&lt;/tt&gt; for
globals.&lt;/p&gt;
&lt;p&gt;It's not possible to convert implicitly the &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt; to a &lt;tt class="docutils literal"&gt;fat.verdict&lt;/tt&gt;,
because the &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt; is expected to be mutated, and the guards be will on
&lt;tt class="docutils literal"&gt;fat.verdict&lt;/tt&gt; not on the original &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;I worked around the bug by creating manually a &lt;tt class="docutils literal"&gt;fat.verdict&lt;/tt&gt; in FAT mode,
instead of a &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;This bug will go avoid if the versionning feature is moved directly into
the builtin &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt; type (and the &lt;tt class="docutils literal"&gt;fat.verdict&lt;/tt&gt; type is removed).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</summary><category term="optimization"></category><category term="fatpython"></category></entry><entry><title>Port your Python 2 applications to Python 3 with sixer</title><link href="https://haypo.github.io/python3-sixer.html" rel="alternate"></link><published>2015-06-16T15:00:00+02:00</published><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2015-06-16:python3-sixer.html</id><summary type="html">&lt;div class="section" id="from-2to3-to-2to6"&gt;
&lt;h2&gt;From 2to3 to 2to6&lt;/h2&gt;
&lt;p&gt;When Python 3.0 was released, the official statement was to port your
application using &lt;a class="reference external" href="https://docs.python.org/3.5/library/2to3.html"&gt;2to3&lt;/a&gt; and
drop Python 2 support. It didn't work because you had to port all libraries
first. If a library drops Python 2 support, existing applications running on
Python 2 cannot use this library anymore.&lt;/p&gt;
&lt;p&gt;This chicken-and-egg issue was solved by the creation of the &lt;a class="reference external" href="https://pythonhosted.org/six/"&gt;six module&lt;/a&gt; by &lt;a class="reference external" href="https://benjamin.pe/"&gt;Benjamin Peterson&lt;/a&gt;. Thank you so much Benjamin! Using the six module, it
is possible to write a single code base working on Python 2 and Python 3.&lt;/p&gt;
&lt;p&gt;2to3 was hacked to create the &lt;a class="reference external" href="http://python-modernize.readthedocs.org/"&gt;modernize&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/limodou/2to6"&gt;2to6&lt;/a&gt; projects to &lt;em&gt;add Python 3 support&lt;/em&gt; without
loosing Python 2 support. Problem solved!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="creation-of-the-sixer-tool"&gt;
&lt;h2&gt;Creation of the sixer tool&lt;/h2&gt;
&lt;p&gt;Problem solved? Well, not for my specific use case. I'm porting the huge
OpenStack project to Python 3. modernize and 2to6 modify a lot of things at
once, add unwanted changes (ex: add &lt;tt class="docutils literal"&gt;from __future__ import absolute_import&lt;/tt&gt;
at the top of each file), and don't respect the OpenStack coding style
(especially the &lt;a class="reference external" href="http://docs.openstack.org/developer/hacking/#imports"&gt;complex rules to sort and group Python imports&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I wrote the &lt;a class="reference external" href="https://pypi.python.org/pypi/sixer"&gt;sixer&lt;/a&gt; project to
&lt;em&gt;generate&lt;/em&gt; patches for OpenStack. The problem is that OpenStack code changes
very quickly, so it's common to have to fix conflicts the day after submiting
a change. At the beginning, it took at least one week to get Python 3 changes
merged, whereas many changes are merged every day, so being able to regenerate
patches helped a lot.&lt;/p&gt;
&lt;p&gt;I created the &lt;a class="reference external" href="https://pypi.python.org/pypi/sixer"&gt;sixer&lt;/a&gt; tool using a list
of regular expressions to replace a pattern with another. For example, it
replaces &lt;tt class="docutils literal"&gt;dict.itervalues()&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;six.itervalues(dict)&lt;/tt&gt;. The code was
very simple.  The most difficult part was to respect the OpenStack coding
style for Python imports.&lt;/p&gt;
&lt;p&gt;sixer is a success since its creationg, it helped me to fix the all obvious
Python 3 issues: replace &lt;tt class="docutils literal"&gt;unicode(x)&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;six.text_type(x)&lt;/tt&gt;, replace
&lt;tt class="docutils literal"&gt;dict.itervalues()&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;six.itervalues(dict)&lt;/tt&gt;, etc. These changes are
simple, but it's boring to have to modify manually many files. The OpenStack
Nova project has almost 1500 Python files for example.&lt;/p&gt;
&lt;p&gt;The development version of sixer supports the following operations:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;all&lt;/li&gt;
&lt;li&gt;basestring&lt;/li&gt;
&lt;li&gt;dict0&lt;/li&gt;
&lt;li&gt;dict_add&lt;/li&gt;
&lt;li&gt;iteritems&lt;/li&gt;
&lt;li&gt;iterkeys&lt;/li&gt;
&lt;li&gt;itertools&lt;/li&gt;
&lt;li&gt;itervalues&lt;/li&gt;
&lt;li&gt;long&lt;/li&gt;
&lt;li&gt;next&lt;/li&gt;
&lt;li&gt;raise&lt;/li&gt;
&lt;li&gt;six_moves&lt;/li&gt;
&lt;li&gt;stringio&lt;/li&gt;
&lt;li&gt;unicode&lt;/li&gt;
&lt;li&gt;urllib&lt;/li&gt;
&lt;li&gt;xrange&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="creation-of-the-sixer-test-suite"&gt;
&lt;h2&gt;Creation of the Sixer Test Suite&lt;/h2&gt;
&lt;p&gt;Slowly, I added more and more patterns to sixer. The code became too complex
to be able to check regressions manually, so I also started to write unit
tests. Now each operation has at least one unit test. Some complex operations
have four tests or more.&lt;/p&gt;
&lt;p&gt;At the beginning, tests called directly the Python function. It is fast and
convenient, but it failed to catch regressions on the command line program.
So I added tests running sixer has a blackbox: pass an input file and check
the output file. Then I added specific tests on the code parsing command line
options.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-new-all-operation"&gt;
&lt;h2&gt;The new &amp;quot;all&amp;quot; operation&lt;/h2&gt;
&lt;p&gt;At the beginning, I used sixer to generate a patch for a single pattern. For
example, replace &lt;tt class="docutils literal"&gt;unicode()&lt;/tt&gt; in a whole project.&lt;/p&gt;
&lt;p&gt;Later, I started to use it differently: I fixed all Python 3 issues at once,
but only in some selected files. I did that when we reached a minimum set of
tests which pass on Python 3 to have a green py34 check on Jenkins. Then we
ported tests one by one. It's better to write short patches, they are easier
and faster to review. And the review process is the bottlebeck of the
OpenStack development process.&lt;/p&gt;
&lt;p&gt;To fix all Python 3 at once, I added an &lt;tt class="docutils literal"&gt;all&lt;/tt&gt; operation which simply applies
sequentially each operation. So &lt;tt class="docutils literal"&gt;sixer&lt;/tt&gt; can now be used as &lt;tt class="docutils literal"&gt;modernize&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;2to6&lt;/tt&gt; to fix all Python 3 issues at once in a whole project.&lt;/p&gt;
&lt;p&gt;I also added the ability to pass filenames instead of having to pass a
directory to modify all files in all subdirectories.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="new-urllib-six-moves-and-stringio-operations"&gt;
&lt;h2&gt;New urllib, six_moves and stringio operations&lt;/h2&gt;
&lt;div class="section" id="urllib"&gt;
&lt;h3&gt;urllib&lt;/h3&gt;
&lt;p&gt;I tried to keep the sixer code simple. But some changes are boring to write,
like replacing &lt;tt class="docutils literal"&gt;urllib&lt;/tt&gt; imports &lt;tt class="docutils literal"&gt;six.moves.urllib&lt;/tt&gt; imports. Python 2 has 3
modules (&lt;tt class="docutils literal"&gt;urllib&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;urllib2&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;urlparse&lt;/tt&gt;), whereas Pytohn 3 uses a
single &lt;tt class="docutils literal"&gt;urllib&lt;/tt&gt; namespace with submodules (&lt;tt class="docutils literal"&gt;urllib.request&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;urllib.parse&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;urllib.error&lt;/tt&gt;). Some Python 2 functions moved to one
submodule, whereas others moved to another submodules. It required to know
well the old and new layout.&lt;/p&gt;
&lt;p&gt;After loosing many hours to write manually patches for &lt;tt class="docutils literal"&gt;urllib&lt;/tt&gt;, I decided
to add a &lt;tt class="docutils literal"&gt;urllib&lt;/tt&gt; operation. In fact, it was so not long to implement it,
compared to the time taken to write patches manually.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="stringio"&gt;
&lt;h3&gt;stringio&lt;/h3&gt;
&lt;p&gt;Handling StringIO is also a little bit tricky because String.StringIO and
String.cStringIO don't have the same performance on Python 2. Producing
patches without killing performances require to pick the right module or
symbol from six: &lt;tt class="docutils literal"&gt;six.StringIO()&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;six.moves.cStringIO.StringIO&lt;/tt&gt; for
example.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="six-moves"&gt;
&lt;h3&gt;six_moves&lt;/h3&gt;
&lt;p&gt;The generic &lt;tt class="docutils literal"&gt;six_moves&lt;/tt&gt; operation replaces various Python 2 imports with
imports from &lt;tt class="docutils literal"&gt;six.moves&lt;/tt&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;BaseHTTPServer&lt;/li&gt;
&lt;li&gt;ConfigParser&lt;/li&gt;
&lt;li&gt;Cookie&lt;/li&gt;
&lt;li&gt;HTMLParser&lt;/li&gt;
&lt;li&gt;Queue&lt;/li&gt;
&lt;li&gt;SimpleHTTPServer&lt;/li&gt;
&lt;li&gt;SimpleXMLRPCServer&lt;/li&gt;
&lt;li&gt;__builtin__&lt;/li&gt;
&lt;li&gt;cPickle&lt;/li&gt;
&lt;li&gt;cookielib&lt;/li&gt;
&lt;li&gt;htmlentitydefs&lt;/li&gt;
&lt;li&gt;httplib&lt;/li&gt;
&lt;li&gt;repr&lt;/li&gt;
&lt;li&gt;xmlrpclib&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="kiss-emit-warnings-instead-of-complex-implementation"&gt;
&lt;h2&gt;KISS: emit warnings instead of complex implementation&lt;/h2&gt;
&lt;p&gt;As I wrote, I tried to keep sixer simple (KISS principle: Keep It Simple,
Stupid). I'm also lazy, I didn't try to write a perfect tool. I don't want to
spend hours on the sixer project.&lt;/p&gt;
&lt;p&gt;When it was too tricky to make a decision or to implement a pattern, sixer
emits &amp;quot;warnings&amp;quot; instead. For example, a warning is emitted on
&lt;tt class="docutils literal"&gt;def next(self):&lt;/tt&gt; to remind that a &lt;tt class="docutils literal"&gt;__next__ = next&lt;/tt&gt; alias is probably
needed on this class for Python 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The sixer tool is incomplete and generates invalid changes. For example, it
replaces patterns in comments, docstrings and strings, whereas usually these
changes don't make sense. But I'm happy because the tool helped me a lot
for to port OpenStack, it saved me hours.&lt;/p&gt;
&lt;p&gt;I hope that the tool will now be useful to others! Don't hesitate to give me
feedback.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="python3"></category><category term="sixer"></category></entry></feed>