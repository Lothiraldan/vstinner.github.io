+++++++++++++++++++++++++++++++++++++++
PEP 393: Flexible String Representation
+++++++++++++++++++++++++++++++++++++++

:date: 2016-02-17 15:00
:tags: cpython
:category: python
:slug: pep393
:authors: Victor Stinner
:summary: PEP 393: Flexible String Representation

Probably very few people noticed that Python 3.3 was deeply reworked to
reimplement completly the Python ``str`` type (Unicode). The goal is to
reduce the memory footprint of Python 3


Unicode in Python 2 and Python <= 3.2
=====================================

Python 3 stores text strings as Unicode, all text strings: variable names,
function names, class names, etc. Until Python 3.2, it was possible to
compile CPython in two modes: "narrow build" preferred on Windows
and "wide build" preferred on other platforms. Check your kind of build
by checking ``sys.maxunicode`` value:

* 65,535 (0xffff) for narrow build
* 1,114,111 (0x10ffff) for wide build

Wide build
----------

The wide build uses a simple format: all characters are stored as 32-bit
unsigned numbers. This format is also called ``UCS4``.

Simple but inefficient. Even if all characters are 7-bit ASCII (Unicode range
U+0000..U+007f), they use 4 bytes per character. The string "hello" takes 20
bytes just for characters.

Narrow build
------------

The narrow build stores characters as UTF-16 units, 16 bit unsigned numbers.
BMP strings (Unicode range U+0000..U+ffff) take 2 bytes per character.  For
example, "hello" takes 10 bytes for characters. Non-BMP characters (range
U+0000..U+10ffff) requires two UTF-16 units and so each non-BMP character takes
4 bytes.

Using UTF-16 units caused bad surprised to users: str[index] returns UTF-16
units which are not characters for non-BMP characters. For example,
``'\U0010ffff'[0]`` returns ``'\udbff`` which is a surrogate pair, not a
character. The surrogate category of Unicode is the range U+D800-U+DFFF (2,048
code points). On wide build, ``'\U0010ffff'[0]`` returns ``'\U0010ffff'``.

To give users a better experience with Unicode on non-BMP characters, Python
used various hacks to iterate on characters rather than iterating on UTF-16
units for narrow builds.  These hacks made the code complex and difficult to
maintain.

By the way, most CPython core developers use Linux and Mac OS X, and few of
them have a working environment to develop on Windows. Since issues with narrow
builds only occur on Windows, only a few cases are well handled, and the code
is full of corner cases.


PEP 393 structures
==================

Compact strings
---------------

In 2010, Martin v. Löwis wrote the `PEP 393 -- Flexible String Representation
<https://www.python.org/dev/peps/pep-0393/>`_ to reduce the Python memory
footprint for Unicode strings.

He proposed to *always* use the most compact storage for characters:

* 1 byte per character (``Py_UCS1``) for ASCII (Unicode range U+0000..U+007f)
  and Latin1 strings (U+0000..U+00ff)
* 2 bytes per character (``Py_UCS2``) for "BMP" strings (U+0000..U+ffff)
* 4 bytes per character (``Py_UCS4``) for non-BMP strings (U+0000..U+10ffff)

To use the most compact C structure for strings, there is not only one C
structure, but 3 structures:

* PyASCIIObject
* PyCompactUnicodeObject
* PyUnicodeObject

In Python 3.2 and older, a Unicode string used two memory blocks (memory
allocations): one for the header, one for characters.

PyASCIIObject and PyCompactUnicodeObject structures store the header and the
characters in the same memory block. Using a single memory block rather than
two is more efficient for the memory allocator, it reduces the memory
fragmentation and it helps data locality (CPU memory caches).


Backward compatibility: build string using Py_UNICODE
-----------------------------------------------------

For backward compatibility, a Unicode string can also be created using the
legacy ``Py_UNICODE*`` format using the ``PyUnicodeObject`` structure:
``Py_UNICODE`` is a 16-bit unsigned number on Windows, or 32-bit unsigned
number on other platforms.

When old code produces a legacy ``Py_UNICODE`` string, it must be converted to
a compact string with
``PyUnicode_READY(str)``. We say that the string becomes "ready".

The PyUnicodeObject structure uses two memory blocks: one block for the header,
one block for characters. The characters can be used as 4 different formats:
``Py_UNICODE`` (legacy), ``Py_UCS1``, ``Py_UCS2`` or ``Py_UCS4``.


Backward compatibility: convert compact string to Py_UNICODE
------------------------------------------------------------

Again, for backward compatibility with existing extension modules which expect
``Py_UNICODE*`` strings, Python can encode a compact string to the
``Py_UNICODE`` format.

The compact string is converted to the legacy ``Py_UNICODE`` format, the result
is cached as a private attribute of the string. The legacy format is stored in
a new memory block. Since ``Py_UNICODE`` is ``Py_UCS2`` (Windows) or
``Py_UCS4`` (other platforms) in practice, the conversion is skipped if the
compact strings already uses ``Py_UNICODE`` format.  In this case, a pointer to
compact data is used, instead of creating a new memory block.


Implementation of the PEP 393
=============================

https://wiki.python.org/moin/SummerOfCode/2011/PEP393

Rewriting the old CPython code base to use the new PEP 393 complex format was a
big project. It was the topic of a Google Summer of Code 2011 with the student
Torsten Becker menthored by Martin v. Löwis (author of the PEP). The project
was successful: the PEP 393 was implemented, it worked!

To be honest, I was *very* skeptical by the feasability of such code since they
are ton of C code relying on CPython implementation details. Martin design was
great since it kept the backward compatibility even if CPython exposes too much
implementation details in its C API.

The first implementation used ``Py_UCS4`` buffers to build strings, and then
converted it to compact format. Simple, but inefficient. Most functions
used the ``PyUnicode_WRITE()`` macro or similar code which uses a switch
on the string kind to use Py_UCS1, Py_UCS2 or Py_UCS4. Used in a loop,
it was inefficient.


Document PEP 393 format
=======================

My first contribution was to document what we had: I added a lot of comments on
PyASCIIObject, PyCompactUnicodeObject and PyUnicodeObject structures in
``Include/unicodeobject.h``.

I added a ``_PyUnicode_CheckConsistency()`` function and added
``assert(_PyUnicode_CheckConsistency(unicode));`` at the end of all functions
building or modifying strings. I spent a lot of time on this tiny function
because properties of the various formats of Unicode strings were not
documented. Obviously, I found and fixed bugs.

I added ``unicode_fill_invalid()`` to fill a string with ``0xff`` bytes:
it helps to detect non initialized characters in ASCII and UCS4 strings
(``0xff`` bytes are valid for UCS1 and UCS2: characters U+00ff and U+ffff).

I added ``unicode_check_modifiable()`` to ensure that a string can still be
modified: it raises a ``SystemError`` exception if the string was already used.
It uses a simple heuristic to detect common mistakes:

* check the reference counter: must be 1
* check if the hash was computed: the hash must be non-initialized
* check if the string was interned (must not be interned)

I added ``unicode_result(str)`` which checks the string consistency and reuses
Latin1 singletons when applicable.

